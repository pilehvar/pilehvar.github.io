<link href="https://fonts.googleapis.com/css2?family=Roboto&display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=Open+Sans:wght@300&family=Roboto&display=swap" rel="stylesheet">
<style type="text/css">
 .tg  {border-collapse:collapse;border-spacing:0;font-family:'Open Sans', sans-serif;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:'Open Sans',sans-serif;font-size:14px;
  overflow:hidden;padding:10px 15px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:'Open Sans',sans-serif;font-size:14px;
  font-weight:bold;overflow:hidden;padding:10px 15px;word-break:normal;}
</style>


<h1>Series 2</h1>
<h2> Summer 2020 </h2>

<table class="tg">
<thead>
  <tr>
    <th>Date</th>
    <th>Moderators</th>
    <th>Topic / Paper</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td>4 July</td>
    <td>Houman Mehrafarin, Hossein Mohebbi, Amin Pourdabiri, Ali Modaresi</td>
    <td><span style="font-weight:normal">Perturbed Masking: Parameter-free Probing for Analyzing and Interpreting BERT</span><br>
        <span style="font-weight:400;font-style:normal">BERT Loses Patience: Fast and Robust Inference with Early Exit</span>
        <span style="font-weight:400;font-style:normal">Accelerating Natural Language Understanding in Task-Oriented Dialog</span>
        <span style="font-weight:400;font-style:normal">Fine-tune BERT with Sparse Self-Attention Mechanism</span></td>
  </tr>
  <tr>
    <td>11 July</td>
    <td>Houman Mehrafarin, Sara Rajaee</td>
    <td><span style="font-weight:normal">Linguistic Knowledge and Transferability of Contextual Representations</span><br>
        <span style="font-weight:400;font-style:normal">Finding Universal Grammatical Relations in Multilingual BERT</span></td>
  </tr>

   <tr>
    <td>15 July</td>
    <td>Mohsen Tabasy, Kiamehr Rezaee, Zahra Sayedi</td>
    <td><span style="font-weight:normal">How does BERTâ€™s attention change when you fine-tune? An analysis methodology and a case study in negation scope</span><br>
        <span style="font-weight:400;font-style:normal">Similarity of Neural Network Representations Revisited</span>
    <span style="font-weight:400;font-style:normal">Consistent Dialogue Generation with Self-supervised Feature Learning</span></td>
  </tr>
 
  <tr>
    <td>22 July</td>
    <td>Hossein Mohebbi, Amin Pourdabiri, Ali Modaresi</td>
    <td><span style="font-weight:normal">BERT-of-Theseus: Compressing BERT by Progressive Module Replacing</span><br>
        <span style="font-weight:400;font-style:normal">Beyond Domain APIs: Task-oriented Conversational Modeling with Unstructured Knowledge Access</span></td>
        <span style="font-weight:400;font-style:normal">FastBERT: a Self-distilling BERT with Adaptive Inference Time</span></td>
  </tr>
  <tr>
   <td>29 July </td>
   <td>-- </td>
   <td>-- </td>
  </tr>
</tbody>
</table>


<h1>Series 1</h1>
<h2> Spring 2020 </h2>

<table class="tg">
<thead>
  <tr>
    <th>Date</th>
    <th>Moderator</th>
    <th>Topic / Paper</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td>20 April</td>
    <td>Mohsen Tabasy</td>
    <td><span style="font-weight:normal">BERT Rediscovers the Classical NLP Pipeline</span><br><span style="font-weight:400;font-style:normal">Universal Adversarial Triggers for Attacking and Analyzing NLP</span></td>
  </tr>
  <tr>
    <td>27 April</td>
    <td>Hossein Mohebbi</td>
    <td>oLMpics -- On what Language Model Pre-training Captures<br>Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping</td>
  </tr>
  <tr>
    <td>4 May</td>
    <td>Houman Mehrafarin</td>
    <td><span style="font-weight:400;font-style:normal">What Does BERT Look At? An Analysis of BERT's Attention</span></td>
  </tr>
  <tr>
    <td>11 May</td>
    <td>Amin Pourdabiri</td>
    <td><span style="font-weight:400;font-style:normal">Spying on your neighbors: Fine-grained probing of contextual embeddings for information about surrounding words</span></td>
  </tr>
  <tr>
    <td>18 May</td>
    <td>Kiamehr Rezaee</td>
    <td><span style="font-weight:400;font-style:normal">How Multilingual is Multilingual BERT?</span></td>
  </tr>
  <tr>
    <td>25 May</td>
    <td>-</td>
    <td>-</td>
  </tr>
  <tr>
    <td>1 June</td>
    <td>Ali Modaresi</td>
    <td>BERT-based Lexical Substitution<br>Contextual Embeddings: When Are They Worth It?</td>
  </tr>
</tbody>
</table>


