<html>
<head>
  <title>NLP Reading Group :: Tehran Institute for Advanced Studies (TeIAS) :: Mohammad Taher Pilehvar</title>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-3E0YK9EXY2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-3E0YK9EXY2');
</script>


<link href="https://fonts.googleapis.com/css2?family=Roboto&display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=Open+Sans:wght@300&family=Roboto&display=swap" rel="stylesheet">
<style type="text/css">
 .tg  {border-collapse:collapse;border-spacing:0;font-family:'Open Sans', sans-serif; width:90%;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:'Open Sans',sans-serif;font-size:14px;
  overflow:hidden;padding:10px 15px;word-break:normal;background-color:WhiteSmoke;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:'Open Sans',sans-serif;font-size:14px;
  font-weight:bold;overflow:hidden;padding:10px 15px;word-break:normal;background-color:lightgray;}
</style>
</head>

<body style="margin:20;padding:20">

<h1>NLP Reading Group</h1>
<h2>Tehran Institute for Advanced Studies (TeIAS)</h2>
<h3>Moderator: Mohammad Taher Pilehvar</h3>
<h3>Wednesdays, 11am-12pm (online on Teams)</h3>
<h4><i>Contact the moderator if you are interested in joining the group.</i></h4>

<hr>


<h2> Summer 2022 </h2>

<table class="tg">
<thead>
  <tr>
    <th width="10%">Date</th>
    <th width="20%">Presenters</th>
    <th width="70%">Topic / Paper</th>
  </tr>
	
 <tr>
 	 <td>31 August</td>
	  <td>Mahdi Zakizadeh</td>
	  <td>
	  <ul>
		  <li>Exploring Gender Bias in Retrieval Models, Arxiv 2022.</li>  
	  </ul>
	  </td>
</tr>

	
<tr>
 	 <td>24 August</td>
	  <td>Zahra Delbari</td>
	  <td>
	  <ul>
		  <li>DACT-BERT: Differentiable Adaptive Computation Time for an Efficient BERT Inference, NLP Power! 2022.</li>
		  <li>DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference, ACL 2020.</li>  
	  </ul>
	  </td>
</tr>

 <tr>
 	 <td>3 August</td>
	  <td>Mahdi Zakizadeh</td>
	  <td>
	  <ul>
		  <li>Don't Forget About Pronouns: Removing Gender Bias in Language Models Without Losing Factual Gender Information, GeBNLP 2022.</li>  
	  </ul>
	  </td>
</tr>

 <tr>
 	 <td>27 July</td>
	  <td>Zahra Delbari</td>
	  <td>
	  <ul>
		  <li>Compacter: Efficient Low-Rank Hypercomplex Adapter Layers, NeurIPS 2021.</li>  
	  </ul>
	  </td>
</tr>

		
 <tr>
 	 <td>13 July</td>
	  <td>Zahra Delbari</td>
	  <td>
	  <ul>
		  <li>A Flexible Multi-Task Model for BERT Serving, ACL 2022.</li>  
	  </ul>
	  </td>
</tr>
	
 <tr>
 	 <td>6 July</td>
	  <td>Kaveh Eskandai</td>
	  <td>
	  <ul>
		  <li>Perturbation Augmentation for Fairer NLP, Arxiv 2022.</li>  
	  </ul>
	  </td>
</tr>
	
 <tr>
 	 <td>29 June</td>
	  <td>Mehrdad Nasser, Mahdi Zakizadeh</td>
	  <td>
	  <ul>
		  <li>Structured Prediction as Translation between Augmented Natural Languages, Arxiv 2021.</li>
		  <li>Fewer Errors, but More Stereotypes? The Effect of Model Size on Gender Bias, GeBNLP 2022.</li>
	  </ul>
	  </td>
</tr>
	

</tbody>
</table>

<br><br>

	
	
	
<h2> Spring 2022 </h2>

<table class="tg">
<thead>
  <tr>
    <th width="10%">Date</th>
    <th width="20%">Presenters</th>
    <th width="70%">Topic / Paper</th>
  </tr>



<tr>
 	 <td>1 June</td>
	 <td>Maryam Sadat Hashemi</td>
	  <td>
	  <ul>
		<li>Worst of Both Worlds: Biases Compound in Pre-trained Vision-and-Language Models, GeBNLP 2022.</li>
	  </ul>
	  </td>
</tr>
	
<tr>
 	 <td>25 May</td>
	 <td><i>ACL 2022</i></td>
	  <td>
	  <ul>

	  </ul>
	  </td>
</tr>
	

<tr>
 	 <td>18 May</td>
	  <td>Mahdi Zakizadeh</td>
	  <td>
	  <ul>
		  <li>How Gender Debiasing Affects Internal Model Representations, and Why It Matters.</li>
	  </ul>
	  </td>
</tr>
	
<tr>
 	 <td>11 May</td>
	  <td>Kaveh Eskandari</td>
	  <td>
	  <ul>
		  <li>OPT: Open Pre-Trained Transformers.</li>
	  </ul>
	  </td>
</tr>
	
<tr>
 	 <td>4 MAy</td>
	  <td>--</td>
	  <td>
	  <ul>

	  </ul>
	  </td>
</tr>
	

<tr>
 	 <td>27 April</td>
	  <td>Mahdi Zakizadeh</td>
	  <td>
	  <ul>
		  <li>Towards a Unified View of Parameter-Efficient Transfer Learning.</li>  
	  </ul>
	  </td>
</tr>
	

<tr>
 	 <td>20 April</td>
	  <td>Mehrdad Nasser, Kaveh ESkandari</td>
	  <td>
	  <ul>
		  <li>Transformer Memory as a Differentiable Search Index.</li>
		  <li>BRIO: Bringing Order to Abstractive Summarization.</li>
		  
	  </ul>
	  </td>
</tr>
	

<tr>
 	 <td>13 April</td>
	  <td>Mohammad Hossein Khojasteh, Maryam Sadat Hashemi</td>
	  <td>
	  <ul>
		  <li>Decomposing Complex Questions Makes Multi-Hop QA Easier and More Interpretable.</li>
		  <li>Vision-and-Language or Vision-for-Language? On Cross-Modal Influence in Multimodal Transformers.</li>
	  </ul>
	  </td>
</tr>

<tr>
 	 <td>6 April</td>
	  <td>Sara Rajaee</td>
	  <td>
	  <ul>
		  <li>Efficient One-Pass End-to-End Entity Linking for Questions.</li>
	  </ul>
	  </td>
</tr>
	
	
<tr>
 	 <td>6 April</td>
	  <td>Sara Rajaee</td>
	  <td>
	  <ul>
		  <li>Efficient One-Pass End-to-End Entity Linking for Questions.</li>
	  </ul>
	  </td>
</tr>
	

</tbody>
</table>

<br><br>

	
	
<h2> Winter 2021 </h2>

<table class="tg">
<thead>
  <tr>
    <th width="10%">Date</th>
    <th width="20%">Presenters</th>
    <th width="70%">Topic / Paper</th>
  </tr>


	
<tr>
 	 <td>16 March</td>
	  <td>Mahdi Zakizadeh, Mehrdad Nasser</td>
	  <td>
	  <ul>
		  <li>Multitask Prompted Training Enables Zero-Shot Task Generalization.</li>
		  <li>Efficient One-Pass End-to-End Entity Linking for Questions.</li>
	  </ul>
	  </td>
</tr>
	

<tr>
 	 <td>9 March</td>
	  <td>Mehrdad Nasser, Kaveh Eskandari</td>
	  <td>
	  <ul>
		  <li>Improving Multi-hop Question Answering over Knowledge Graphs using Knowledge Base Embeddings.</li>
		  <li>Challenges in Detoxifying Language Models.</li>
	  </ul>
	  </td>
</tr>
	

<tr>
 	 <td>2 March</td>
	  <td>Houman Mehrafarin, Maryam Sadat Hashemi</td>
	  <td>
	  <ul>
		  <li>Negated and Misprimed Probes for Pretrained Language Models: Birds Can Talk, But Cannot Fly.</li>
		  <li>Decoupling the Role of Data, Attention, and Losses in Multimodal Transformers.</li>
	  </ul>
	  </td>
</tr>
	
<tr>
 	 <td>23 February</td>
	  <td>Mohammad Hossein Khojasteh, Mohammad Ali Modarressi</td>
	  <td>
	  <ul>
		  <li>Reinforced History Backtracking for Conversational Question Answering.</li>
		  <li>What do Compressed Large Language Models Forget? Robustness Challenges in Model Compression.</li>
	  </ul>
	  </td>
</tr>
	
<tr>
 	 <td>16 February</td>
	  <td>Mohammad Hossein Khojasteh, Mahdi Zakizadeh</td>
	  <td>
	  <ul>
		  <li>QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering.</li>
		  <li>Assessing the Reliability of Word Embedding Gender Bias Measures.</li>
	  </ul>
	  </td>
</tr>
	
	
<tr>
 	 <td>9 February</td>
	  <td>Kaveh Eskandari, Sara Rajaee</td>
	  <td>
	  <ul>
		  <li>Do Transformers Encode a Foundational Ontology? Probing Abstract Classes in Natural Language.</li>
		  <li>Information-Theoretic Measures of Dataset Difficulty.</li>
	  </ul>
	  </td>
</tr>
	

	
<tr>
 	 <td>2 February</td>
	  <td>Zahra Dehghani, Houman Mehrafarin</td>
	  <td>
	  <ul>
		  <li>Don't Search for a Search Method—Simple Heuristics Suffice for Adversarial Text Attacks.</li>
		  <li>Contributions of Transformer Attention Heads in Multi- and Cross-lingual Tasks.</li>
	  </ul>
	  </td>
</tr>
	

<tr>
 	 <td>26 January</td>
	  <td>Mehrdad Nasser, Kaveh Eskandari</td>
	  <td>
	  <ul>
		  <li>REALM: Retrieval-Augmented Language Model Pre-Training.</li>
		  <li>Simple, Interpretable and Stable Method for Detecting Words with Usage Change Across Corpora.</li>
	  </ul>
	  </td>
</tr>
	
<tr>
 	 <td>19 January</td>
	  <td>Maryam Sadat Hashemi, Amin Pourdabiri</td>
	  <td>
	  <ul>
		  <li>Good Prompt Is Worth Millions of Parameters? Low-resource Prompt-based Learning for Vision-Language Models.</li>
		  <li>MEDCOD: A Medically-Accurate, Emotive, Diverse, and COntrollable Dialog System.</li>
	  </ul>
	  </td>
</tr>
	
<tr>
 	 <td>12 January</td>
	  <td>Mehrdad Nasser, Sara Rajaee</td>
	  <td>
	  <ul>
		  <li>Knowledgeable or Educated Guess? Revisiting Language Models as Knowledge Bases.</li>
		  <li>Towards Interpreting and Mitigating Shortcut Learning Behavior of NLU Models.</li>
	  </ul>
	  </td>
</tr>
	
<tr>
 	 <td>5 January</td>
	  <td>Zahra Dehghani, Mohammad Hossein Khojasteh</td>
	  <td>
	  <ul>
		  <li>Model Extraction and Adversarial Transferability, Your BERT is Vulnerable!</li>
		  <li>Neural Unification for Logic Reasoning over Natural Language.</li>
	  </ul>
	  </td>
</tr>

<tr>
 	 <td>29 December</td>
	  <td>Houman Mehrafarin, Maryam Sadat Hashemi</td>
	  <td>
	  <ul>
		  <li>Introducing Orthogonal Constraint in Structural Probes.</li>
		  <li>SimVLM Simple Visual Language Model Pretraining with Weak Supervision</li>
	  </ul>
	  </td>
</tr>

<tr>
 	 <td>22 December</td>
	  <td>Rabeeh Karimi Mahabadi</td>
	  <td>
	  <ul>
		<li>PERFECT: Prompt-free and Efficient Language Model Fine-Tuning, Arxiv 2021.</li>
	  </ul>
	  </td>
</tr>

<tr>
 	 <td>22 December</td>
	  <td>Sara Rajaee, Mohammad Ali Modarressi</td>
	  <td>
	  <ul>
		<li>Transformer Feed-Forward Layers Are Key-Value Memories, EMNLP 2021.</li>
		<li>Learning from others' mistakes: Avoiding dataset biases without modeling them, ICLR 2021.</li>
	  </ul>
	  </td>
</tr>


</tbody>
</table>

<br><br>






<h2> Fall 2021 </h2>

<table class="tg">
<thead>
  <tr>
    <th width="10%">Date</th>
    <th width="20%">Presenters</th>
    <th width="70%">Topic / Paper</th>
  </tr>



<tr>
 	 <td>15 December</td>
	  <td>Kaveh Eskandari</td>
	  <td>
	  <ul>
		<li>Words of Wisdom: Representational Harms in Learning From AI Communication, LearnTec4EDI 2021.</li>
		<li>Evaluating Debiasing Techniques  for Intersectional Biases, EMNLP 2021.</li>
	  </ul>
	  </td>
</tr>

<tr>
 	 <td>8 December</td>
	  <td>Fangyu Liu</td>
	  <td>
	  <ul>
		<li>Visually Grounded Reasoning across Languages and Cultures, EMNLP 2021.</li>
	  </ul>
	  </td>
</tr>

<tr>
 	 <td>24 November</td>
	  <td>Mohammad Hossein Khojasteh, Houman Mehrafarin</td>
	  <td>
	  <ul>
		<li>Editing Factual Knowledge in Language Models, EMNLP 2021.</li>
		<li>Factual Probing Is [MASK]: Learning vs. Learning to Recall, NAACL 2021.</li>
	  </ul>
	  </td>
</tr>

<tr>
 	 <td>17 November</td>
	  <td>Zahra Sayedi, Maryam Sadat Hashemi</td>
	  <td>
	  <ul>
		<li>Augmenting Data for Sarcasm Detection with Unlabeled Conversation Context, FigLang, ACL 2020.</li>
		<li>Effect of Visual Extensions on Natural Language Understanding in Vision-and-Language Models, EMNLP 2021.</li>
	  </ul>
	  </td>
</tr>

<tr>
 	 <td>10 November</td>
	  <td>Zahra Dehghani</td>
	  <td>
	  <ul>
		<li>Universal Adversarial Attacks with Natural Triggers for Text Classification, NAACL 2021. [<a href="https://drive.google.com/file/d/1W8rAO-usBIVFJkZ0UyDFKE3l1xYBLJzP/view?usp=sharing" target="#">slides</a>]
</li>
	  </ul>
	  </td>
</tr>

<tr>
 	 <td>3 November</td>
	  <td>Mahdi Zakizadeh, Amin Pourdabiri</td>
	  <td>
	  <ul>
		<li>Fairness without Demographics through Adversarially Reweighted Learning, NeurIPS 2020. [<a href="https://drive.google.com/file/d/1xrWhxycqwkjT0tfurWTM-UcBS2iTWgJO/view?usp=sharing" target="#">slides</a>]</li>
		<li>An animated picture says at least a thousand words: Selecting Gif-based Replies in Multimodal Dialog, Findings of EMNLP 2021. [<a href="https://drive.google.com/file/d/1s8IIbLHdzrNY3Ed-Eyy1JcZWC0FWIK9K/view?usp=sharing" target="#">slides</a>]</li>

	  </ul>
	  </td>
</tr>

<tr>
 	 <td>27 October</td>
	  <td>Kaveh Eskandari, Sara Rajaee</td>
	  <td>
	  <ul>
		<li>Grokking: Generalization Beyond Overfitting On Small Algorithmic Data Sets, MathAI, ICLR 2021.</li>
		<li>EBERT: Efficient BERT Inference with Dynamic Structured Pruning, Findings of ACL 2021.</li>
	  </ul>
	  </td>
</tr>

<tr>
 	 <td>20 October</td>
	  <td>Amin Pourdabiri, Mohammad Ali Modarressi</td>
	  <td>
	  <ul>
		<li>Enriching Pre-trained Language Model with Entity Information for Relation Classification, Arxiv 2019.</li>
		<li>Beyond Preserved Accuracy: Evaluating Loyalty and Robustness of BERT Compression, EMNLP 2021.</li>
	  </ul>
	  </td>
</tr>

<tr>
 	 <td>13 October</td>
	  <td>Houman Mehrafarin, Maryam Sadat Hashemi</td>
	  <td>
	  <ul>
		<li>Multilingual BERT Post-Pretraining Alignment, NAACL 2021. </li>
		<li>Multimodal Few-Shot Learning with Frozen Language Models, Arxiv 2021.</li>
	  </ul>
	  </td>
</tr>


<tr>
 	 <td>6 October</td>
	  <td>Zahra Dehghani, Mahdi Zakizadeh</td>
	  <td>
	  <ul>
		<li>Style is NOT a single variable: Case Studies for Cross-Style Language Understanding, ACL 2021. </li>
		<li>Mitigating Unwanted Biases with Adversarial Learning, Arxiv 2018. [<a href="https://drive.google.com/file/d/1AiNnGp26euNhtcNI4qkJ1yR0KtFe4rzl/view?usp=sharing" target="#">slides</a>]</li>
	  </ul>
	  </td>
</tr>


<tr>
 	 <td>29 September</td>
	  <td>Kaveh Eskandari, Mohammad Ali Modarressi</td>
	  <td>
	  <ul>
		<li>Are Gender-Neutral Queries Really Gender-Neutral? Mitigating Gender Bias in Image Search, EMNLP 2021. [<a href="https://drive.google.com/file/d/1Mgu2FyaXlu-fP-pyv5frZhiT4DjM8De2/view?usp=sharing" target="#">slides</a>]</li>
		<li>Fine-Tuned Transformers Show Clusters of Similar Representations Across Layers, BlackboxNLP 2021.</li>
	  </ul>
	  </td>
</tr>

<tr>
 	 <td>22 September</td>
	  <td>Tohid Abedini, Houman Mehrafarin</td>
	  <td>
	  <ul>
		<li>Constituency Parsing with a Self-Attentive Encoder, ACL 2018. </li>
		<li>Discourse Probing of Pretrained Language Models, NAACL 2021.</li>
	  </ul>
	  </td>
</tr>


</tbody>
</table>

<br><br>



<h2> Summer 2021 </h2>

<table class="tg">
<thead>
  <tr>
    <th width="10%">Date</th>
    <th width="20%">Presenters</th>
    <th width="70%">Topic / Paper</th>
  </tr>


<tr>
 	 <td>15 September</td>
	  <td>Sara Rajaee, Maryam Sadat Hashemi</td>
	  <td>
	  <ul>
		<li>Too Much in Common: Shifting of Embeddings in Transformer Language Models and its Implications, NAACL 2021. [<a href="https://drive.google.com/file/d/1p5UT6O3Ue-DAYJZd54_1S4d8gdl-8r5l/view?usp=sharing" target="#">slides</a>]</li>
		<li>All Bark and No Bite: Rogue Dimensions in Transformer Language Models Obscure Representational Quality, EMNLP 2021. </li>
		<li>Finetuned Language Models Are Zero-Shot Learners, Arxiv 2021. [<a href="https://drive.google.com/file/d/1OxDDNZ_NGRSx3xSKXe1atw66ufBDTI5X/view?usp=sharing" target="#">slides</a>]</li>
	  </ul>
	  </td>
</tr>


<tr>
 	 <td>8 September</td>
	  <td>Hosein Mohebbi, Zahra Sayedi</td>
	  <td>
	  <ul>
		<li>Enjoy the Salience: Towards Better Transformer-based Faithful Explanations with Word Salience, EMNLP 2021. [<a href="https://drive.google.com/file/d/13dWsbpPHK3k7pTMEwBbrhm209nwEN8-A/view?usp=sharing" target="#">slides</a>] </li>
		<li>BoB: BERT Over BERT for Training Persona-based Dialogue Models, ACL 2021.</li>
	  </ul>
	  </td>
</tr>

<tr>
 	 <td>1 September</td>
	  <td>Mahdi Zakizadeh, Amin Pourdabiri</td>
	  <td>
	  <ul>
		<li>On Transferability of Bias Mitigation Effects in Language Model Fine-Tuning, NAACL 2021.</li>
		<li>Towards Emotional Support Dialog Systems, ACL 2021. [<a href="https://drive.google.com/file/d/16NZKKXRUkEhgpFXN2s-VgGnB4SiSjp-i/view?usp=sharing" target="#">slides</a>] </li>
	  </ul>
	  </td>
</tr>

<tr>
 	 <td>18 August</td>
	  <td> <i> National holiday </i> </td>
	  <td> -- </td>
</tr>

<tr>
 	 <td>11 August</td>
	  <td>Hosein Mohebbi, Mohsen Tabasi</td>
	  <td>
	  <ul>
		<li>Probing BERT in Hyperbolic Spaces, ICLR 2021. [<a href="https://drive.google.com/file/d/1XeWy2dTSjsgc3uwXm9NYCVFqq1ZUwQor/view?usp=sharing" target="#">slides</a>] </li>
		<li>Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning, ACL 2021.</li>
	  </ul>
	  </td>
</tr>

<tr>
 	 <td>4 August</td>
	  <td>Tohid Abedini, Mohammad Ali Modarressi</td>
	  <td>
	  <ul>
		<li>Combating Adversarial Misspellings with Robust Word Recognition, ACL 2019. [<a href="https://drive.google.com/file/d/1IEAzAxOJbcxmvn1hn1AOsQEvkMuQzixl/view?usp=sharing" target="#">slides</a>] </li>
		<li>On Attention Redundancy: A Comprehensive Study, NAACL 2021.</li>
	  </ul>
	  </td>
</tr>


<tr>
 	 <td>28 July</td>
	  <td>Houman Mehrafarin, Maryam Sadat Hashemi</td>
	  <td>
	  <ul>
		<li>An Empirical Study on Robustness to Spurious Correlations using Pre-trained Language Models, TACL 2020.</li>
		<li>Charformer: Fast Character Transformers via Gradient-based Subword Tokenization, Arxiv 2021. [<a href="https://drive.google.com/file/d/1VYH3NGYl5mrww597u0t9IQWVBGqFEWJz/view?usp=sharing" target="#">slides</a>]</li>
	  </ul>
	  </td>
</tr>

<tr>
 	 <td>21 July</td>
	  <td> <i> National holiday </i> </td>
	  <td> -- </td>
</tr>

 <tr>
	<td>14 July</td>
	  <td>Kiamehr Rezaee, Zahra Sayedi</td>
	  <td>
	  <ul>
		<li>Generationary or: “How We Went beyond Word Sense Inventories and Learned to Gloss”, EMNLP 2020.</li>
		<li>All That’s ‘Human’ Is Not Gold: Evaluating Human Evaluation of Generated Text, ACL 2021.</li>
	  </ul>
	  </td>
  </tr>

  <tr>
	<td>7 July</td>
	  <td>Hosein Mohebbi, Mohsen Tabasi</td>
	  <td>
	  <ul>
		  <li>MPNet: Masked and Permuted Pre-training for Language Understanding , NeurIPS 2020. [<a href="https://drive.google.com/file/d/1dBneFNMwgNR1N-lgebUQqPAdtwx3TzSn/view?usp=sharing" target="#">slides</a>] </li>
		  <li>Comparing Test Sets with Item Response Theory, ACL 2021.</li>
	  </ul>
	  </td>
  </tr>

  <tr>
	<td>30 June</td>
	  <td>Amin Pourdabiri, Sara Rajaee</td>
	  <td>
	  <ul>
		  <li>SIMMC 2.0: A Task-oriented Dialog Dataset for Immersive Multimodal Conversations, Arxiv 2021. [<a href="https://drive.google.com/file/d/1xL_OMmTKX6jQ1FsstMND6xymsTTF9bg1/view?usp=sharing" target="#">slides</a>] </li>
		  <li>Positional Artefacts Propagate Through Masked Language Model Embeddings, ACL 2021. [<a href="https://drive.google.com/file/d/1i4kDeRM0kU5qkbaLo9ETHfQWu32_YxSS/view?usp=sharing" target="#">slides</a>] </li>
	  </ul>
	  </td>
  </tr>


  <tr>
	<td>23 June</td>
	  <td>Houman Mehrafarin, Maryam Sadat Hashemi</td>
	  <td>
	  <ul>
		  <li>Intermediate-Task Transfer Learning with Pretrained Models for Natural Language Understanding: When and Why Does It Work? ACL 2020.</li>
		  <li>Unsupervised Vision-and-Language Pre-training Without Parallel Images and Captions, NAACL 2021. [<a href="https://drive.google.com/file/d/1oxAUEAwD7DBVI6tDSE5ZHAFWqC55S4V6/view?usp=sharing" target="#">slides</a>]</li>
	  </ul>
	  </td>
  </tr>

</tbody>
</table>

<br><br>

<h2> Spring 2021 </h2>

<table class="tg">
<thead>
  <tr>
    <th width="10%">Date</th>
    <th width="20%">Presenters</th>
    <th width="70%">Topic / Paper</th>
  </tr>


 <tr>
	<td>16 June</td>
	  <td>Hosein Mohebbi, Sara Rajaee</td>
	  <td>
	  <ul>
		  <li>TR-BERT: Dynamic Token Reduction for Accelerating BERT Inference, NAACL 2021. [<a href="https://drive.google.com/file/d/1yKoZObxFjs_YZe46iOYhCRntt5sOurKc/view?usp=sharing" target="#">slides</a>]</li>
		  <li>How transfer learning impacts linguistic knowledge in deep NLP models? ACL 2021. [<a href="https://drive.google.com/file/d/1oufjUyEI3S2EEOmDVJHntutQHaiJpmTm/view?usp=sharing" target="#">slides</a>]</li>
	  </ul>
	  </td>
  </tr>


 <tr>
	<td>9 June</td>
	  <td>Mohsen Tabasi, Mohammad Ali Modarressi</td>
	  <td>
	  <ul>
		  <li>Entailment as Few-Shot Learner, Arxiv 2021.</li>
		  <li>BERT Busters: Outlier LayerNorm Dimensions that Disrupt Transformers, Findings of ACL 2021.</li>
	  </ul>
	  </td>
  </tr>

 <tr>
	<td>2 June</td>
	  <td>Sara Rajaee, Maryam Sadat Hashemi</td>
	  <td>
	  <ul>
		  <li>A Cluster-based Approach for Improving Isotropy in Contextual Embedding Space, ACL 2021.</li>
		  <li>FNet: Mixing Tokens with Fourier Transforms, Arxiv 2021. [<a href="https://drive.google.com/file/d/1SNJ5Rwp4uxk7ZeFJQuSgbABZ4ILK5ARG/view?usp=sharing" target="#">slides</a>]</li>
	  </ul>
	  </td>
  </tr>


 <tr>
	<td>26 May</td>
	  <td>Kiamehr Rezaee</td>
	  <td>
	  <ul>
		  <li>Beyond Fine-tuning: Few-Sample Sentence Embedding Transfer, ACL 2020.</li>
	  </ul>
	  </td>
  </tr>

 <tr>
	<td>19 May</td>
	  <td>Amin Pourdabiri</td>
	  <td>
	  <ul>
		  <li>Situated and Interactive Multimodal Conversations, COLING 2020. [<a href="https://drive.google.com/file/d/1_uz8GNMz2b1r1sf1aHovpcWCSKAyHMRT/view?usp=sharing" target="#">slides</a>]</li>
	  </ul>
	  </td>
  </tr>


  <tr>
	<td>12 May</td>
	  <td> <i>EMNLP deadline</i> </td>
	  <td> --  </td>
  </tr>

  <tr>
	<td>5 May</td>
	  <td>Zahra Sayedi, Houman Mehrafarin</td>
	  <td>
	  <ul>
		  <li>Sequential Latent Knowledge Selection for Knowledge-Grounded Dialogue, ICLR 2020.</li>
		  <li>What Happens To BERT Embeddings During Fine Tuning, BlackboxNLP 2020.</li>
	  </ul>
	  </td>
  </tr>

  <tr>
	<td>28 April</td>
	  <td>Hosein Mohebbi, Mohammad Ali Modarressi</td>
	  <td>
	  <ul>
		  <li>DirectProbe: Studying Representations without Classifiers, NAACL 2021. [<a href="https://drive.google.com/file/d/1N7wer9L5VrBPJ4RbuX_ErdaMi0gAkjji/view?usp=sharing" target="#">slides</a>]</li>
		  <li>Telling BERT’s Full Story: from Local Attention to Global Aggregation, EACL 2021.</li>
	  </ul>
	  </td>
  </tr>
	
  <tr>
	<td>21 April</td>
	  <td> Mohsen Tabasi, Zahra Sayedi</td>
	  <td>
	  <ul>
		  <li>Static Embeddings as Efficient Knowledge Bases? NAACL 2021.</li>
		  <li>Towards a Human-like Open-Domain Chatbot, Arxiv 2020.</li>
	  </ul>
	  </td>
  </tr>
	
  <tr>
	<td>14 April</td>
	  <td> Amin Pourdabiri, Sara Rajaee</td>
	  <td>
	  <ul>
		  <li>Emotion Dynamics in Movie Dialogues, Arxiv 2021. [<a href="https://drive.google.com/file/d/1509yrjCA4CDxFFAEdXiqnwUsIreneb35/view?usp=sharing" target="#">slides</a>]</li>
		  <li>Infusing Finetuning with Semantic Dependencies, TACL 2021.</li>
	  </ul>
	  </td>
  </tr>
	
  <tr>
	<td>7 April</td>
	  <td> Kiamehr Rezaee, Houman Mehrafarin</td>
	  <td>
	  <ul>
		  <li>AUTOPROMPT: Eliciting Knowledge from Language Models with Automatically Generated Prompts, EMNLP 2020.</li>
		  <li>Making Pre-trained Language Models Better Few-shot Learners, Arxiv 2020.</li>
		  <li>Probing What Different NLP Tasks Teach Machines about Function Word Comprehension, *SEM 2019.</li>
	  </ul>
	  </td>
  </tr>
	
</tbody>
</table>

<br><br>

<h2> Winter 2020/2021 </h2>

<table class="tg">
<thead>
  <tr>
    <th width="10%">Date</th>
    <th width="20%">Presenters</th>
    <th width="70%">Topic / Paper</th>
  </tr>
  <tr>
	<td>17 March</td>
	  <td> Hosein Mohebbi, Mohammad Ali Modarressi</td>
	  <td>
	  <ul>
		<li>Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference. EACL 2021. [<a href="https://drive.google.com/file/d/1-0l8hc05IMZ41nmcksL5SIpsDB2HK9X8/view?usp=sharing" target="#">slides</a>]</li>
		<li>How Many Data Points is a Prompt Worth?. NAACL 2021.</li>
		<li>It's Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners (Arxiv 2020).</li>
	  </ul>
	  </td>
  </tr>

  <tr>
	<td>10 March</td>
	  <td> Mohsen Tabasi, Sara Rajaee</td>
	  <td>
	  <ul>
		<li>Interpretation of NLP models through input marginalization. EMNLP 2020.</li>
		<li>Designing and Interpreting Probes with Control Tasks. EMNLP 2019.</li>
	  </ul>
	  </td>
  </tr>

  <tr>
	<td>3 March</td>
	  <td> Amin Pourdabiri, Sara Rajaee</td>
	  <td>
	  <ul>
		<li>Generate, Delete and Rewrite: A Three-Stage Framework for Improving Persona Consistency of Dialogue Generation. ACL 2020. [<a href="https://drive.google.com/file/d/1lrXqACxMOyo99Q3sDZsr92ne_G3eZIhB/view?usp=sharing" target="#">slides</a>]</li>
		<li>Analyzing Individual Neurons in Pre-trained Language Models. EMNLP 2020. [<a href="https://drive.google.com/file/d/1-QR99wOTVmAcmt4B2ctukwiYZpTrOmej/view?usp=sharing" target="#">slides</a>]</li>
	  </ul>
	  </td>
  </tr>

  <tr>
	<td>24 February</td>
	  <td> Houman Mehrafarin, Kiamehr Rezaee</td>
	  <td>
	  <ul>
		<li>Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning. Rep4NLP 2020.</li>
		<li>On Identifiability in Transformers. ICLR 2020.</li>
	  </ul>
	  </td>
  </tr>

  <tr>
	<td>17 February</td>
	  <td> Hosein Mohebbi, Mohammad Ali Modarressi</td>
	  <td>
	  <ul>
		<li>Amnesic Probing: Behavioral Explanation with Amnesic Counterfactuals. TACL 2021. [<a href="https://drive.google.com/file/d/105v97XIIj0UAKMA1G8inB11xkKF_xKnm/view?usp=sharing" target="#">slides</a>]</li>
		<li>First Align, then Predict: Understanding the Cross-Lingual Ability of Multilingual BERT. EACL 2021.</li>
	  </ul>
	  </td>
  </tr>

  <tr>
	<td>10 February</td>
	  <td> <i> National holiday </i> </td>
	  <td> -- </td>
  </tr>

  <tr>
	<td>3 February</td>
	  <td> Mohsen Tabasi, Mahsa Razavi </td>
	  <td>
	  <ul>
		<li>Generalization through Memorization: Nearest Neighbor Language Models. ICLR 2020.</li>
		<li>SMRT Chatbots: Improving Non-Task-Oriented Dialog with Simulated Multiple Reference Training. EMNLP 2020. [<a href="https://drive.google.com/file/d/1QTsYIQMQb_IVDFofKFtJQikxmte3GH0R/view?usp=sharing" target="#">slides</a>]</li></li>
	  </ul>
	  </td>
  </tr>

  <tr>
	<td>27 January</td>
	  <td> <i>ACL deadline</i> </td>
	  <td> --  </td>
  </tr>

  <tr>
	<td>20 January</td>
	  <td> Amin Pourdabiri, Sara Rajaee </td>
	  <td>
	  <ul>
		<li>Fine-grained Emotion and Intent Learning in Movie Dialogues (Arxiv 2020). [<a href="https://drive.google.com/file/d/10mTszLxVYRrC4G54EkQYFoEvMY3k4mmn/view?usp=sharing" target="#">slides</a>]</li>
		<li>Unsupervised Distillation of Syntactic Information from Contextualized Word Representations. BlackBoxNLP 2020. [<a href="https://drive.google.com/file/d/1F8CU4IABVzpoywHNLpc_AScvXleV1Gh2/view?usp=sharing" target="#">slides</a>]</li>
	  </ul>
	  </td>
  </tr>
  <tr>
	<td>13 January</td>
	  <td> Houman Mehrafarin, Maryam Sadat Hashemi </td>
	  <td>
	  <ul>
		<li>Attention Interpretability Across NLP Tasks (Arxiv 2019).</li>
		<li>When BERT Plays the Lottery, All Tickets Are Winning. EMNLP 2020. [<a href="https://drive.google.com/file/d/1vNLyUXDjINZZtmdcWY9uLSbnISl0znvo/view?usp=sharing" target="#">slides</a>]</li>
	  </ul>
	  </td>
  </tr>
  
  <tr>
	<td>6 January (2021)</td>
	  <td> Kiamehr Rezaee, Mohammad Ali Modarressi </td>
	  <td>
	  <ul>
	  <li>Attention is Not Only Weight: Analyzing Transformers with Vector Norms. EMNLP 2020.</li>
 	  <li>How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models. </li>
	  </ul>
	  </td>
  </tr>
  
  <tr>
	<td>30 December</td>
	  <td> Mohsen Tabasi, Hosein Mohebbi </td>
	  <td>
	  <ul>
	  <li>Experience Grounds Language. EMNLP 2020.</li>
 	  <li>Intrinsic Probing through Dimension Selection. EMNLP 2020. [<a href="https://drive.google.com/file/d/1ve2eLxftwSUDwlvR5Kv5S0jCH6-a2wLl/view?usp=sharing" target="#">slides</a>]</li>
	  </ul>
	  </td>
  </tr>
  
  <tr>
	<td>23 December</td>
	  <td> <i>Student project proposals</i> </td>
	  <td> --
	  </td>
  </tr>


</tbody>
</table>


<h2> Fall 2020 </h2>

<table class="tg">
<thead>
  <tr>
    <th width="10%">Date</th>
    <th width="20%">Presenters</th>
    <th width="70%">Topic / Paper</th>

  <tr>
	<td>16 December</td>
	  <td>Sara Rajaee, Mahsa Razavi</td>
	  <td>
	  <ul>
	  <li>Topology of Word Embeddings: Singularities Reflect Polysemy. *SEM 2020. [<a href="https://drive.google.com/file/d/1tjV012uBQzUXLlUTbJRk2Bs83l3ByYD5/view?usp=sharing" target="#">slides</a>]</li>
 	  <li>Diversifying Dialogue Generation with Non-Conversational Text. ACL 2020.
 	  [<a href="https://drive.google.com/file/d/1BTL6ES4GbQR3a224irLhn0eFx4zABioI/view?usp=sharing" target="#">slides</a>]
 	  </li>
	  </ul>
	  </td>
  </tr>
  
  <tr>
	<td>9 December</td>
	  <td>Houman Mehrafarin, Amin Pourdabiri</td>
	  <td>
	  <ul>
	  <li>Assessing BERT's Syntactic Abilities. 2019.</li>
 	  <li>Personal Information Leakage Detection in Conversations. EMNLP 2020. [<a href="https://drive.google.com/file/d/1Gjcqrcc3hNuvgA6D6Hwxl0ZiemQegYge/view?usp=sharing" target="#">slides</a>]
 	  </li>
	  </ul>
	  </td>
  </tr>
  
  <tr>
	<td>2 December</td>
	  <td>Maryam Sadat Hashemi, Kiamehr Rezaee</td>
	  <td>
	  <ul>
	  <li>Vokenization: Improving Language Understanding with Contextualized, Visual-Grounded Supervision. EMNLP 2020. [<a href="https://drive.google.com/file/d/1VApjvjkddRe3ZZW3LJNjPH5qOmKuJaH5/view?usp=sharing" target="#">slides</a>]</li>
 	  <li>Linguistic Profiling of a Neural Language Model. COLING 2020.</li>
	  </ul>
	  </td>
  </tr>

  <tr>
	<td>25 November</td>
	  <td>Hosein Mohebbi, Mohsen Tabasi, Sara Rajaee</td>
	  <td>
	  <ul>
	  <li>The elephant in the interpretability room. Why use attention as explanation when we have saliency methods? BlackboxNLP 2020. [<a href="https://drive.google.com/file/d/1STOKFrgcBi5AOc2ajSxSDkrFBxb3OHzM/view?usp=sharing" target="#">slides</a>]</li>
 	  <li>How do Decisions Emerge across Layers in Neural Models? Interpretation with Differentiable Masking. EMNLP 2020.</li>
	  <li>Asking without Telling: Exploring Latent Ontologies in Contextual Representations. EMNLP 2020. [<a href="https://drive.google.com/file/d/1jgphQUXTigt8iollQ-4GiVpKWM18SYWQ/view?usp=sharing" target="#">slides</a>]</li>
	  </ul>
	  </td>
  </tr>

  <tr>
	<td>18 November</td>
	  <td>Mohammad Ali Modarressi, Samin Fatehi</td>
	  <td>
	  <ul>
	  <li>Pretrained Language Model Embryology: The Birth of ALBERT. EMNLP 2020.</li>
 	  <li>ETC: Encoding Long and Structured Inputs in Transformers. EMNLP 2020. [<a href="https://drive.google.com/file/d/1SOjcJAljOceQnxggG3wbVFNYRLbjVltd/view?usp=sharing" target="#">slides</a>]</li>
	  </ul>
	  </td>
  </tr>


  <tr>
	<td>11 November</td>
	  <td>Amin Pourdabiri, Mahsa Razavi</td>
	  <td>
	  <ul>
	  <li>Hierarchical Reinforcement Learning for Open-Domain Dialog. AAAI 2020. [<a href="https://drive.google.com/file/d/1fvNsRm9oOYm0Ps5StHIFoMGBNCR0G8qB/view?usp=sharing" target="#">slides</a>]</li>
 	  <li>Filtering Noisy Dialogue Corpora by Connectivity and Content Relatedness. EMNLP 2020. [<a href="https://drive.google.com/file/d/1zGaCYNy4U_ITkFj8H4x0BTtJ40FCrOEZ/view?usp=sharing" target="#">slides</a>]</li>
	  </ul>
	  </td>
  </tr>

  <tr>
	<td>4 November</td>
	  <td>Houman Mehrafarin, Kiamehr Rezaee, Maryam Sadat Hashemi</td>
	  <td>
	  <ul>
	  <li>Are Sixteen Heads Really Better Than One? NIPS 2019. [<a href="https://drive.google.com/file/d/1HCJB2TVtIqwWu-MMHo07oscsn57ff365/view?usp=sharing" target="#">slides</a>]</li>
 	  <li>Do Explicit Alignments Robustly Improve Multilingual Encoders. EMNLP 2020.</li>
	  <li>Rethinking attention with performers. [<a href="https://drive.google.com/file/d/17iN_wceIKdaPpU1hG3UNDjtkYj7lwE8x/view?usp=sharing" target="#">slides</a>]</li>
	  </ul>
	  </td>
  </tr>

  <tr>
	<td>28 October</td>
	  <td>Hosein Mohebbi, Mohammad Ali Modarressi</td>
	  <td>
	  <ul>
	  <li>A Tale of a Probe and a Parser. ACL 2020.</li>
 	  <li>What Do Position Embeddings Learn? An Empirical Study of Pre-Trained Language Model Positional Encoding. EMNLP 2020.</li>
	  </ul>
	  </td>
  </tr>

  <tr>
	<td>21 October</td>
	  <td>Sara Rajaee, Samin Fatehi, Mahsa Razavi</td>
	  <td>
	  <ul>
	  <li>BERT-EMD: Many-to-Many Layer Mapping for BERT Compression with Earth Mover's Distance. EMNLP 2020. [<a href="https://drive.google.com/file/d/1JUcs9BVayf9uTEIId-2yelL0sYqRRAx2/view?usp=sharing" target="#">slides</a>]</li>
 	  <li>Null It Out: Guarding Protected Attributes by Iterative Nullspace Projection. ACL 2020.</li>
	  <li>Multi-Turn Response Selection for Chatbots with Deep Attention Matching Network. ACL 2018.</li>
	  </ul>
	  </td>
  </tr>

  <tr>
	<td>14 October</td>
	  <td>Sara Rajaee, Maryam Sadat Hashemi</td>
	  <td>
	  <ul>
	  <li>Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them. NAACL 2019. [<a href="https://drive.google.com/file/d/17e_MlncTiuZrDFg8WA1SzwEwdO9sEh1B/view?usp=sharing" target="#">slides</a>]</li>
 	  <li>Cross-Modality Relevance for Reasoning on Language and Vision. ACL 2020. [<a href="https://drive.google.com/file/d/1tzWB07Oiyl7dZReSx6_Kgg1jBbNpDkia/view?usp=sharing" target="#">slides</a>]</li>
	  </ul>
	  </td>
  </tr>

  <tr>
	<td>7 October</td>
	  <td>Houman Mehrafarin, Kiamehr Rezaee, Mohsen Tabasi</td>
	  <td>
	  <ul>
	  <li>What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models. TACL 2020. [<a href="https://drive.google.com/file/d/1xeDb9Fu3tD1WaR95t1LC-UMiu4coazcx/view?usp=sharing" target="#">slides</a>]</li>
 	  <li>Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference. </li>
          <li>Breaking Through the 80% Glass Ceiling: Raising the State of the Art in Word Sense Disambiguation by Incorporating Knowledge Graph Information. ACL 2020.</li>
	  </ul>
	  </td>
  </tr>

  <tr>
	<td>30 September</td>
	  <td>Amin Pourdabiri, Maryam Sadat Hashemi, Samin Fatehi</td>
	  <td>
	  <ul>
	  <li>Big Bird: Transformers for Longer Sequences. [<a href="https://drive.google.com/file/d/1Ggvxa3TRkYenDXJJt7xeYDd1VmgOSE0G/view?usp=sharing" target="#">slides</a>]</li>
 	  <li>Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks. ECCV 2020. [<a href="https://drive.google.com/file/d/1voqJSWtLFzwtsyMiV0dbwY5QVdCsWcj5/view?usp=sharing" target="#">slides</a>]</li>
	  <li>Attention over Parameters for Dialogue Systems. NeurIPS 2020 ConvAI workshop. [<a href="https://drive.google.com/file/d/1QbdUsAOV7RyM6sur4gMIKIa-93xJ2c0e/view?usp=sharing" target="#">slides</a>]</li>
	  </ul>
	  </td>
  </tr>
   
  <tr>
	<td>23 September</td>
	  <td>Ali Modarressi and Hosein Mohebbi</td>
	  <td>
	  <ul>
	  <li>Quantifying Attention Flow in Transformers. ACL 2020. [<a href="https://drive.google.com/file/d/1la5ONPD2Ueg8Hx72OdvkWA5QYQ-9OokM/view?usp=sharing" target="#">slides</a>]</li>
 	  <li>DeFormer: Decomposing Pre-trained Transformers for Faster Question Answering. ACL 2020. [<a href="https://drive.google.com/file/d/1Q_1xdbiRXDylmNAZCkBKxZs99SJ3D4Rc/view?usp=sharing" target="#">slides</a>]</li>
	  </ul>
	  </td>

  </tr>

</tbody>
</table>

<h2> Summer 2020 </h2>

<table class="tg">
<thead>
  <tr>
    <th width="10%">Date</th>
    <th width="20%">Presenters</th>
    <th width="70%">Topic / Paper</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td>4 July</td>
    <td>Houman Mehrafarin, Hosein Mohebbi, Amin Pourdabiri, Ali Modarressi</td>
    <td>
     <ul>
        <li>Perturbed Masking: Parameter-free Probing for Analyzing and Interpreting BERT</li>
        <li>BERT Loses Patience: Fast and Robust Inference with Early Exit. [<a href="https://drive.google.com/file/d/1DKKOqFpJfJd8ontlwq3oYHqkLbkFB2Fp/view?usp=sharing" target="#">slides</a>]</li>
        <li>Accelerating Natural Language Understanding in Task-Oriented Dialog</li>
        <li>Fine-tune BERT with Sparse Self-Attention Mechanism. [<a href="https://drive.google.com/file/d/16M5_q7cX5p_1pvqteWYUBDb5v2P3KdbF/view?usp=sharing" target="#">slides</a>]</li>
     </ul>
     </td>
  </tr>
  <tr>
    <td>11 July</td>
    <td>Houman Mehrafarin, Sara Rajaee</td>
   <td><ul><li>Linguistic Knowledge and Transferability of Contextual Representations. [<a href="https://drive.google.com/file/d/1srXeXF_3mKJb73XtS67TBQqxh1h3utXx/view?usp=sharing" target="#">slides</a>]</li>
    <li>Finding Universal Grammatical Relations in Multilingual BERT. [<a href="https://drive.google.com/file/d/1RxMu8lVRPxZIm7c0bZCYxEDf8KqCVPr5/view?usp=sharing" target="#">slides</a>] </li></ul></td>
  </tr>

   <tr>
    <td>15 July</td>
    <td>Mohsen Tabasi, Kiamehr Rezaee, Zahra Sayedi</td>
    <td><ul><li>How does BERT’s attention change when you fine-tune? An analysis methodology and a case study in negation scope. [<a href="https://drive.google.com/file/d/1iVjamQ09LffYPcyu6Y9JwuLUUGf4FGFm/view?usp=sharing" target="#">slides</a>]</li>
        <li>Similarity of Neural Network Representations Revisited</li>
    <li>Consistent Dialogue Generation with Self-supervised Feature Learning</li></ul></td>
  </tr>
 
  <tr>
    <td>22 July</td>
    <td>Hosein Mohebbi, Amin Pourdabiri, Ali Modarressi</td>
   <td><ul><li>BERT-of-Theseus: Compressing BERT by Progressive Module Replacing. [<a href="https://drive.google.com/file/d/1pHjNqun2PoRhn0Ar84D_v62G3BBPY7zR/view?usp=sharing" target="#">slides</a>]</li>
        <li>Beyond Domain APIs: Task-oriented Conversational Modeling with Unstructured Knowledge Access</li>
        <li>FastBERT: a Self-distilling BERT with Adaptive Inference Time. [<a href="https://drive.google.com/file/d/1swoewrBClZywNwyEAOK3fUNE6t3WJ-9o/view?usp=sharing" target="#">slides</a>]</li></td>
  </tr>
  <tr>
   <td>29 July </td>
   <td>Sara Rajaee, Maryam Sadat Hashemi, Samin Fatehi</td>
   <td><ul><li>On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines. [<a href="https://drive.google.com/file/d/1Wr8RECTpn5iGHHQ9FrKNcxR6Kkdicuhs/view?usp=sharing" taret="#">slides</a>]</li>
	   <li>VisualBERT: A Simple and Performant Baseline for Vision and Language. [<a href="https://drive.google.com/file/d/1890RMx5gfc7LePE0rx46KYpxaHNmzYkv/view?usp=sharing" taret="#">slides</a>]</li>
	   <li>Parameter-Efficient Transfer Learning for NLP. [<a href="https://drive.google.com/file/d/17L9ar4q-hR-Tb6-by0vJ7b-iL4T9uFNN/view?usp=sharing" taret="#">slides</a>]</li></ul></td>
  </tr>
  <tr>
	<td>5 August</td>
	  <td>Mohsen Tabasi, Kiamehr Rezaee, Zahra Sayedi</td>
	  <td><ul>
		<li>Explaining Black Box Predictions and Unveiling Data Artifacts through Influence Functions. [<a href="https://drive.google.com/file/d/1v3aSnIPSQbirkUOhYhYO3lP8rfO9b55_/view?usp=sharing" target="#">slides</a>]</li>
		<li>Beyond Accuracy: Behavioral Testing of NLP Models with CheckList</li>
		<li>You Impress Me: Dialogue Generation via Mutual Persona Perception</li>
		  </ul></td>
  </tr>
  <tr>
	  <td>12 August</td>
	  <td>Houman Mehrafarin, Hosein Mohebbi, Mohammad Ali Modarresi</td>
	  <td>
	  <ul>
	  <li>Probing Linguistic Systematicity, ACL 2020. [<a href="https://drive.google.com/file/d/1vm2yFV61tjw0bi90oXS0nOMJvW2lv41X/view?usp=sharing" target="#">slides</a>]</li>
	  <li>ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators, ICLR 2020. [<a href="https://drive.google.com/file/d/1epybyJdJpNjsm1yzIaRHzhgV6Q6omffX/view?usp=sharing" target="#">slides</a>]</li>
	  <li>BART: Denoising Sequence-to-Sequence Pre-training for Natural
Language Generation, Translation, and Comprehension, ACL 2020. [<a href="https://drive.google.com/file/d/1YMjdzQwwPXVQPhqmynfaU2J0A8Uh1eJZ/view?usp=sharing" target="#">slides</a>]</li>
	</ul>
	  </td>
  </tr>
    <tr>
	<td>19 August</td>
	  <td>Sara Rajaee, Samin Fatehi, Mahsa Razavi</td>
	  <td><ul>
		<li>A Mixture of h-1 Heads is Better than h Heads, ACL 2020. [<a href="https://drive.google.com/file/d/1-0JD8-ozDScxz9Lq1GPHuGaSs4wo8jof/view?usp=sharing" target="#">slides</a>]</li>
		<li>Longformer: The Long-Document Transformer, 2020. [<a href="https://drive.google.com/file/d/1zoYEHU8Q3KeNKc7YNGRiVt9tgNi-V8rm/view?usp=sharing" target="#">slides</a>]</li>
		<li>Transferable Multi-Domain State Generator for Task-Oriented Dialogue Systems, ACL 2019.</li>
	</ul></td>
  </tr>
     <tr>
	<td>26 August</td>
	  <td>Mohsen Tabasi, Kiamehr Rezaee, Amin Pourdabiri</td>
	  <td>
	  <ul>
	  <li>The Sensitivity of Language Models and Humans to Winograd Schema Perturbations, ACL 2020. [<a href="https://drive.google.com/file/d/18xYH-Q25ijBvRDbWQWgjNftYNLbdldYV/view?usp=sharing" target="#">slides</a>]
	  [<a href="https://github.com/m-tabasy/nlp_notebooks/blob/master/Inspect_mlm.ipynb" target="#">notebook</a>]
	  </li>
 	  <li>Learning to Speak and Act in a Fantasy Text Adventure Game. EMNLP 2019. [<a href="https://drive.google.com/file/d/1jImPx7btT_Axkxc2Nwtl_Nozv-AlRz4q/view?usp=sharing" target="#">slides</a>]</li>
  	  <li>Emerging Cross-lingual Structure in Pretrained Language Models. ACL 2020.</li>
	  </ul>
	  </td>
  </tr>
  
  <tr>
	<td>2 September</td>
	  <td>Amin Pourdabiri, Maryam Sadat Hashemi, Samin Fatehi</td>
	  <td>
	  <ul>
	  <li>Deploying Lifelong Open-Domain Dialogue Learning. [<a href="https://drive.google.com/file/d/13Uh_-VUfULMbw5AqZGgs6gJ3lGBkzMYh/view?usp=sharing" target="#">slides</a>]</li>
 	  <li>Reformer: The Efficient Transformer, ICLR 2020. [<a href="https://drive.google.com/file/d/1VItxdJzGghR5F3ihUEwvY2_MNpB1JoFg/view?usp=sharing" target="#">slides</a>]</li>
  	  <li>Revealing the Dark Secrets of BERT, EMNLP 2019. [<a href="https://drive.google.com/file/d/1jzfaRDIx0188O_a-he0AEJY-FU34JGw5/view?usp=sharing" target="#">slides</a>]</li>
	  </ul>
	  </td>
  </tr> 

  <tr>
	<td>9 September</td>
	  <td>Houman Mehrafarin, Kiamehr Rezaee</td>
	  <td>
	  <ul>
	  <li>Do Neural Language Models Show Prefrences for Syntactic Formalisms? ACL 2020. [<a href="https://drive.google.com/file/d/1y60DgzLxAUEsDtLLLDiamnMPrrxZmp1Y/view?usp=sharing" target="#">slides</a>]</li>
 	  <li>Multilingual Alignment of Contextual Word Representations. ICLR 2020.</li>
	  </ul>
	  </td>
  </tr>

  <tr>
	<td>16 September</td>
	  <td>Mohsen Tabasi, Maryam Sadat Hashemi, Sara Rajaee</td>
	  <td>
	  <ul>
	  <li>Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data. ACL 2020. [<a href="https://drive.google.com/file/d/1NZ31ldxOJNLoTS9fKMV2ENRNmcj79v59/view?usp=sharing" target="#">slides</a>]</li>
 	  <li>MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices. ACL 2020. [<a href="https://drive.google.com/file/d/1Hk74grw-XwFIoXl6KIRxrWYR7kzH2V30/view?usp=sharing" target="#">slides</a>]</li>
 	  <li>Knowledge Enhanced Contextual Word Representations. EMNLP 2019. [<a href="https://drive.google.com/file/d/1gJFaLAmBzxv8hCP2_XkgNoN8HK9OyrxJ/view?usp=sharing" target="#">slides</a>]</li>
	  </ul>
	  </td>
  </tr>
    
</tbody>
</table>

<br>

<h1>Series 1</h1>
<h2> Spring 2020 </h2>

<table class="tg">
<thead>
  <tr>
    <th width="10%">Date</th>
    <th width="20%">Presenters</th>
    <th width="70%">Topic / Paper</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td>20 April</td>
    <td>Mohsen Tabasi</td>
    <td>
    <ul><li>BERT Rediscovers the Classical NLP Pipeline. [<a href="https://drive.google.com/file/d/12TZyuArjU5EaCPshbOBH9RvdY0LT67G5/view?usp=sharing" taret="#">slides</a>]</li>
    <li>Universal Adversarial Triggers for Attacking and Analyzing NLP. [<a href="https://drive.google.com/file/d/12TZyuArjU5EaCPshbOBH9RvdY0LT67G5/view?usp=sharing" taret="#">slides</a>]</li>
    </ul></td>
  </tr>
  <tr>
    <td>27 April</td>
    <td>Hosein Mohebbi</td>
    <td>
    <ul><li>
    oLMpics -- On what Language Model Pre-training Captures
    </li>
    <li>Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping. [<a href="https://drive.google.com/file/d/10h6tOj-ceQfUNwcC4KynFllvDSRj-IWq/view?usp=sharing" target="#">slides</a>]</li></ul></td>
  </tr>
  <tr>
    <td>4 May</td>
    <td>Houman Mehrafarin</td>
    <td><ul><li>
    What Does BERT Look At? An Analysis of BERT's Attention. [<a href="https://drive.google.com/file/d/1ikvjY38DuyTM8uqKYZ0_Dw8nB-DCZXiy/view?usp=sharing" target="#">slides</a>]</li></ul></td>
  </tr>
  <tr>
    <td>11 May</td>
    <td>Amin Pourdabiri</td>
    <td><ul><li>Spying on your neighbors: Fine-grained probing of contextual embeddings for information about surrounding words</li></ul></td>
  </tr>
  <tr>
    <td>18 May</td>
    <td>Kiamehr Rezaee</td>
    <td><ul><li>How Multilingual is Multilingual BERT?</li></ul></td>
  </tr>
  <tr>
    <td>25 May</td>
    <td>-</td>
    <td>-</td>
  </tr>
  <tr>
    <td>1 June</td>
    <td>Ali Modarressi</td>
    <td>
    <ul>
    <li>BERT-based Lexical Substitution. [<a href="https://drive.google.com/file/d/11ztPI63a6VifHi_zhtNilYsb3ZoV75h8/view?usp=sharing" target="#">slides</a>]</li>
    <li>Contextual Embeddings: When Are They Worth It? [<a href="https://drive.google.com/file/d/1ww2ulinYZ4QlSCpiR87uzO_kweYaxfOH/view?usp=sharing" target="#">slides</a>]</li>
    </ul>
    </td>
  </tr>
</tbody>
</table>


