<link href="https://fonts.googleapis.com/css2?family=Roboto&display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=Open+Sans:wght@300&family=Roboto&display=swap" rel="stylesheet">
<style type="text/css">
 .tg  {border-collapse:collapse;border-spacing:0;font-family:'Open Sans', sans-serif;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:'Open Sans',sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:'Open Sans',sans-serif;font-size:14px;
  font-weight:bold;overflow:hidden;padding:10px 5px;word-break:normal;}
</style>


<h1>Series 1</h1> - <h2> Spring 2020 </h2>

<table class="tg">
<thead>
  <tr>
    <th>Date</th>
    <th>Moderator</th>
    <th>Topic / Paper</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-s1k4">20 April</td>
    <td class="tg-s1k4">Mohsen Tabasy</td>
    <td class="tg-35wu"><span style="font-weight:normal">BERT Rediscovers the Classical NLP Pipeline</span><br><span style="font-weight:400;font-style:normal">Universal Adversarial Triggers for Attacking and Analyzing NLP</span></td>
  </tr>
  <tr>
    <td class="tg-s1k4">27 April</td>
    <td class="tg-s1k4">Hossein Mohebbi</td>
    <td class="tg-35wu">oLMpics -- On what Language Model Pre-training Captures<br>Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping</td>
  </tr>
  <tr>
    <td class="tg-s1k4">4 May</td>
    <td class="tg-s1k4">Houman Mehrafarin</td>
    <td class="tg-s1k4"><span style="font-weight:400;font-style:normal">What Does BERT Look At? An Analysis of BERT's Attention</span></td>
  </tr>
  <tr>
    <td class="tg-s1k4">11 May</td>
    <td class="tg-s1k4">Amin Pourdabiri</td>
    <td class="tg-s1k4"><span style="font-weight:400;font-style:normal">Spying on your neighbors: Fine-grained probing of contextual embeddings for information about surrounding words</span></td>
  </tr>
  <tr>
    <td class="tg-s1k4">18 May</td>
    <td class="tg-s1k4">Kiamehr Rezaee</td>
    <td class="tg-s1k4"><span style="font-weight:400;font-style:normal">How Multilingual is Multilingual BERT?</span></td>
  </tr>
  <tr>
    <td class="tg-s1k4">25 May</td>
    <td class="tg-s1k4">-</td>
    <td class="tg-s1k4">-</td>
  </tr>
  <tr>
    <td class="tg-s1k4">1 June</td>
    <td class="tg-s1k4">Ali Modaresi</td>
    <td class="tg-s1k4">BERT-based Lexical Substitution<br>Contextual Embeddings: When Are They Worth It?</td>
  </tr>
</tbody>
</table>
