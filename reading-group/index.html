<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-s1k4{font-family:Georgia, serif !important;;text-align:left;vertical-align:top}
.tg .tg-95lf{font-family:Georgia, serif !important;;font-weight:bold;text-align:left;vertical-align:top}
.tg .tg-35wu{font-family:Georgia, serif !important;;text-align:left;vertical-align:bottom}
</style>


<h1>Series 1</h1> - <h2> Spring 2020 </h2>

<table class="tg">
<thead>
  <tr>
    <th class="tg-95lf">Date</th>
    <th class="tg-95lf">Moderator</th>
    <th class="tg-95lf">Topic / Paper</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-s1k4">20 April</td>
    <td class="tg-s1k4">Mohsen Tabasy</td>
    <td class="tg-35wu"><span style="font-weight:normal">BERT Rediscovers the Classical NLP Pipeline</span><br><span style="font-weight:400;font-style:normal">Universal Adversarial Triggers for Attacking and Analyzing NLP</span></td>
  </tr>
  <tr>
    <td class="tg-s1k4">27 April</td>
    <td class="tg-s1k4">Hossein Mohebbi</td>
    <td class="tg-35wu">oLMpics -- On what Language Model Pre-training Captures<br>Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping</td>
  </tr>
  <tr>
    <td class="tg-s1k4">4 May</td>
    <td class="tg-s1k4">Houman Mehrafarin</td>
    <td class="tg-s1k4"><span style="font-weight:400;font-style:normal">What Does BERT Look At? An Analysis of BERT's Attention</span></td>
  </tr>
  <tr>
    <td class="tg-s1k4">11 May</td>
    <td class="tg-s1k4">Amin Pourdabiri</td>
    <td class="tg-s1k4"><span style="font-weight:400;font-style:normal">Spying on your neighbors: Fine-grained probing of contextual embeddings for information about surrounding words</span></td>
  </tr>
  <tr>
    <td class="tg-s1k4">18 May</td>
    <td class="tg-s1k4">Kiamehr Rezaee</td>
    <td class="tg-s1k4"><span style="font-weight:400;font-style:normal">How Multilingual is Multilingual BERT?</span></td>
  </tr>
  <tr>
    <td class="tg-s1k4">25 May</td>
    <td class="tg-s1k4">-</td>
    <td class="tg-s1k4">-</td>
  </tr>
  <tr>
    <td class="tg-s1k4">1 June</td>
    <td class="tg-s1k4">Ali Modaresi</td>
    <td class="tg-s1k4">BERT-based Lexical Substitution<br>Contextual Embeddings: When Are They Worth It?</td>
  </tr>
</tbody>
</table>
