<html>
<head>
  <title>NLP Reading Group :: IUST :: Mohammad Taher Pilehvar</title>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-3E0YK9EXY2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-3E0YK9EXY2');
</script>


<link href="https://fonts.googleapis.com/css2?family=Roboto&display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=Open+Sans:wght@300&family=Roboto&display=swap" rel="stylesheet">
<style type="text/css">
 .tg  {border-collapse:collapse;border-spacing:0;font-family:'Open Sans', sans-serif; width:90%;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:'Open Sans',sans-serif;font-size:14px;
  overflow:hidden;padding:10px 15px;word-break:normal;background-color:WhiteSmoke;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:'Open Sans',sans-serif;font-size:14px;
  font-weight:bold;overflow:hidden;padding:10px 15px;word-break:normal;background-color:lightgray;}
</style>
</head>

<body style="margin:20;padding:20">

<h1>NLP Reading Group</h1>
<h2>Iran University of Science and Technology</h2>
<h3>Moderator: Mohammad Taher Pilehvar</h3>
<h3>Wednesdays, 11am-12pm (online on Teams)</h3>
<h4><i>Contact the moderator if you are interested in joining the group.</i></h4>

<hr>


<h2> Spring 2021 </h2>

<table class="tg">
<thead>
  <tr>
    <th width="10%">Date</th>
    <th width="20%">Presenters</th>
    <th width="70%">Topic / Paper</th>
  </tr>

  <tr>
	<td>28 April</td>
	  <td>Hosein Mohebbi, Mohammad Ali Modaresi</td>
	  <td>
	  <ul>
		  <li>DirectProbe: Studying Representations without Classifiers, NAACL 2021.</li>
		  <li>Telling BERTâ€™s Full Story: from Local Attention to Global Aggregation, EACL 2021.</li>
	  </ul>
	  </td>
  </tr>
	
  <tr>
	<td>21 April</td>
	  <td> Mohsen Tabasy, Zahra Sayedi</td>
	  <td>
	  <ul>
		  <li>Static Embeddings as Efficient Knowledge Bases? NAACL 2021.</li>
		  <li>Towards a Human-like Open-Domain Chatbot, Arxiv 2020.</li>
	  </ul>
	  </td>
  </tr>
	
  <tr>
	<td>14 April</td>
	  <td> Amin Pourdabiri, Sara Rajaee</td>
	  <td>
	  <ul>
		  <li>Emotion Dynamics in Movie Dialogues, Arxiv 2021.</li>
		  <li>Infusing Finetuning with Semantic Dependencies, TACL 2021.</li>
	  </ul>
	  </td>
  </tr>
	
  <tr>
	<td>7 April</td>
	  <td> Kiamehr Rezaee, Houman Mehrafarin</td>
	  <td>
	  <ul>
		  <li>AUTOPROMPT: Eliciting Knowledge from Language Models with Automatically Generated Prompts, EMNLP 2020.</li>
		  <li>Making Pre-trained Language Models Better Few-shot Learners, Arxiv 2020.</li>
		  <li>Probing What Different NLP Tasks Teach Machines about Function Word Comprehension, *SEM 2019.</li>
	  </ul>
	  </td>
  </tr>
	
</tbody>
</table>

<br><br>

<h2> Winter 2020/2021 </h2>

<table class="tg">
<thead>
  <tr>
    <th width="10%">Date</th>
    <th width="20%">Presenters</th>
    <th width="70%">Topic / Paper</th>
  </tr>
  <tr>
	<td>17 March</td>
	  <td> Hossein Mohebbi, Mohammad Ali Modaresi</td>
	  <td>
	  <ul>
		<li>Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference. EACL 2021. [<a href="https://drive.google.com/file/d/1-0l8hc05IMZ41nmcksL5SIpsDB2HK9X8/view?usp=sharing" target="#">slides</a>]</li>
		<li>How Many Data Points is a Prompt Worth?. NAACL 2021.</li>
		<li>It's Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners (Arxiv 2020).</li>
	  </ul>
	  </td>
  </tr>

  <tr>
	<td>10 March</td>
	  <td> Mohsen Tabasi, Sara Rajaee</td>
	  <td>
	  <ul>
		<li>Interpretation of NLP models through input marginalization. EMNLP 2020.</li>
		<li>Designing and Interpreting Probes with Control Tasks. EMNLP 2019.</li>
	  </ul>
	  </td>
  </tr>

  <tr>
	<td>3 March</td>
	  <td> Amin Pourdabiri, Sara Rajaee</td>
	  <td>
	  <ul>
		<li>Generate, Delete and Rewrite: A Three-Stage Framework for Improving Persona Consistency of Dialogue Generation. ACL 2020. [<a href="https://drive.google.com/file/d/1lrXqACxMOyo99Q3sDZsr92ne_G3eZIhB/view?usp=sharing" target="#">slides</a>]</li>
		<li>Analyzing Individual Neurons in Pre-trained Language Models. EMNLP 2020. [<a href="https://drive.google.com/file/d/1-QR99wOTVmAcmt4B2ctukwiYZpTrOmej/view?usp=sharing" target="#">slides</a>]</li>
	  </ul>
	  </td>
  </tr>

  <tr>
	<td>24 February</td>
	  <td> Houman Mehrafarin, Kiamehr Rezaee</td>
	  <td>
	  <ul>
		<li>Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning. Rep4NLP 2020.</li>
		<li>On Identifiability in Transformers. ICLR 2020.</li>
	  </ul>
	  </td>
  </tr>

  <tr>
	<td>17 February</td>
	  <td> Hossein Mohebbi, Mohammad Ali Modaresi</td>
	  <td>
	  <ul>
		<li>Amnesic Probing: Behavioral Explanation with Amnesic Counterfactuals. TACL 2021. [<a href="https://drive.google.com/file/d/105v97XIIj0UAKMA1G8inB11xkKF_xKnm/view?usp=sharing" target="#">slides</a>]</li>
		<li>First Align, then Predict: Understanding the Cross-Lingual Ability of Multilingual BERT. EACL 2021.</li>
	  </ul>
	  </td>
  </tr>

  <tr>
	<td>10 February</td>
	  <td> <i> National holiday </i> </td>
	  <td> -- </td>
  </tr>

  <tr>
	<td>3 February</td>
	  <td> Mohsen Tabasy, Mahsa Razavi </td>
	  <td>
	  <ul>
		<li>Generalization through Memorization: Nearest Neighbor Language Models. ICLR 2020.</li>
		<li>SMRT Chatbots: Improving Non-Task-Oriented Dialog with Simulated Multiple Reference Training. EMNLP 2020. [<a href="https://drive.google.com/file/d/1QTsYIQMQb_IVDFofKFtJQikxmte3GH0R/view?usp=sharing" target="#">slides</a>]</li></li>
	  </ul>
	  </td>
  </tr>

  <tr>
	<td>27 January</td>
	  <td> <i>ACL deadline</i> </td>
	  <td> --  </td>
  </tr>

  <tr>
	<td>20 January</td>
	  <td> Amin Pourdabiri, Sara Rajaee </td>
	  <td>
	  <ul>
		<li>Fine-grained Emotion and Intent Learning in Movie Dialogues (Arxiv 2020). [<a href="https://drive.google.com/file/d/10mTszLxVYRrC4G54EkQYFoEvMY3k4mmn/view?usp=sharing" target="#">slides</a>]</li>
		<li>Unsupervised Distillation of Syntactic Information from Contextualized Word Representations. BlackBoxNLP 2020. [<a href="https://drive.google.com/file/d/1F8CU4IABVzpoywHNLpc_AScvXleV1Gh2/view?usp=sharing" target="#">slides</a>]</li>
	  </ul>
	  </td>
  </tr>
  <tr>
	<td>13 January</td>
	  <td> Houman Mehrafarin, Maryam Sadat Hashemi </td>
	  <td>
	  <ul>
		<li>Attention Interpretability Across NLP Tasks (Arxiv 2019).</li>
		<li>When BERT Plays the Lottery, All Tickets Are Winning. EMNLP 2020.</li>
	  </ul>
	  </td>
  </tr>
  
  <tr>
	<td>6 January (2021)</td>
	  <td> Kiamehr Rezaee, Mohammad Ali Modaresi </td>
	  <td>
	  <ul>
	  <li>Attention is Not Only Weight: Analyzing Transformers with Vector Norms. EMNLP 2020.</li>
 	  <li>How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models. </li>
	  </ul>
	  </td>
  </tr>
  
  <tr>
	<td>30 December</td>
	  <td> Mohsen Tabasy, Hossein Mohebbi </td>
	  <td>
	  <ul>
	  <li>Experience Grounds Language. EMNLP 2020.</li>
 	  <li>Intrinsic Probing through Dimension Selection. EMNLP 2020. [<a href="https://drive.google.com/file/d/1ve2eLxftwSUDwlvR5Kv5S0jCH6-a2wLl/view?usp=sharing" target="#">slides</a>]</li>
	  </ul>
	  </td>
  </tr>
  
  <tr>
	<td>23 December</td>
	  <td> <i>Student project proposals</i> </td>
	  <td> --
	  </td>
  </tr>


</tbody>
</table>


<h2> Fall 2020 </h2>

<table class="tg">
<thead>
  <tr>
    <th width="10%">Date</th>
    <th width="20%">Presenters</th>
    <th width="70%">Topic / Paper</th>

  <tr>
	<td>16 December</td>
	  <td>Sara Rajaee, Mahsa Razavi</td>
	  <td>
	  <ul>
	  <li>Topology of Word Embeddings: Singularities Reflect Polysemy. *SEM 2020. [<a href="https://drive.google.com/file/d/1tjV012uBQzUXLlUTbJRk2Bs83l3ByYD5/view?usp=sharing" target="#">slides</a>]</li>
 	  <li>Diversifying Dialogue Generation with Non-Conversational Text. ACL 2020.
 	  [<a href="https://drive.google.com/file/d/1BTL6ES4GbQR3a224irLhn0eFx4zABioI/view?usp=sharing" target="#">slides</a>]
 	  </li>
	  </ul>
	  </td>
  </tr>
  
  <tr>
	<td>9 December</td>
	  <td>Houman Mehrafarin, Amin Pourdabiri</td>
	  <td>
	  <ul>
	  <li>Assessing BERT's Syntactic Abilities. 2019.</li>
 	  <li>Personal Information Leakage Detection in Conversations. EMNLP 2020. [<a href="https://drive.google.com/file/d/1Gjcqrcc3hNuvgA6D6Hwxl0ZiemQegYge/view?usp=sharing" target="#">slides</a>]
 	  </li>
	  </ul>
	  </td>
  </tr>
  
  <tr>
	<td>2 December</td>
	  <td>Maryam Sadat Hashemi, Kiamehr Rezaee</td>
	  <td>
	  <ul>
	  <li>Vokenization: Improving Language Understanding with Contextualized, Visual-Grounded Supervision. EMNLP 2020. [<a href="https://drive.google.com/file/d/1VApjvjkddRe3ZZW3LJNjPH5qOmKuJaH5/view?usp=sharing" target="#">slides</a>]</li>
 	  <li>Linguistic Profiling of a Neural Language Model. COLING 2020.</li>
	  </ul>
	  </td>
  </tr>

  <tr>
	<td>25 November</td>
	  <td>Hossein Mohebbi, Mohsen Tabasy, Sara Rajaee</td>
	  <td>
	  <ul>
	  <li>The elephant in the interpretability room. Why use attention as explanation when we have saliency methods? BlackboxNLP 2020. [<a href="https://drive.google.com/file/d/1STOKFrgcBi5AOc2ajSxSDkrFBxb3OHzM/view?usp=sharing" target="#">slides</a>]</li>
 	  <li>How do Decisions Emerge across Layers in Neural Models? Interpretation with Differentiable Masking. EMNLP 2020.</li>
	  <li>Asking without Telling: Exploring Latent Ontologies in Contextual Representations. EMNLP 2020. [<a href="https://drive.google.com/file/d/1jgphQUXTigt8iollQ-4GiVpKWM18SYWQ/view?usp=sharing" target="#">slides</a>]</li>
	  </ul>
	  </td>
  </tr>

  <tr>
	<td>18 November</td>
	  <td>Mohammad Ali Modaresi, Samin Fatehi</td>
	  <td>
	  <ul>
	  <li>Pretrained Language Model Embryology: The Birth of ALBERT. EMNLP 2020.</li>
 	  <li>ETC: Encoding Long and Structured Inputs in Transformers. EMNLP 2020. [<a href="https://drive.google.com/file/d/1SOjcJAljOceQnxggG3wbVFNYRLbjVltd/view?usp=sharing" target="#">slides</a>]</li>
	  </ul>
	  </td>
  </tr>


  <tr>
	<td>11 November</td>
	  <td>Amin Pourdabiri, Mahsa Razavi</td>
	  <td>
	  <ul>
	  <li>Hierarchical Reinforcement Learning for Open-Domain Dialog. AAAI 2020. [<a href="https://drive.google.com/file/d/1fvNsRm9oOYm0Ps5StHIFoMGBNCR0G8qB/view?usp=sharing" target="#">slides</a>]</li>
 	  <li>Filtering Noisy Dialogue Corpora by Connectivity and Content Relatedness. EMNLP 2020. [<a href="https://drive.google.com/file/d/1zGaCYNy4U_ITkFj8H4x0BTtJ40FCrOEZ/view?usp=sharing" target="#">slides</a>]</li>
	  </ul>
	  </td>
  </tr>

  <tr>
	<td>4 November</td>
	  <td>Houman Mehrafarin, Kiamehr Rezaee, Maryam Sadat Hashemi</td>
	  <td>
	  <ul>
	  <li>Are Sixteen Heads Really Better Than One? NIPS 2019. [<a href="https://drive.google.com/file/d/1HCJB2TVtIqwWu-MMHo07oscsn57ff365/view?usp=sharing" target="#">slides</a>]</li>
 	  <li>Do Explicit Alignments Robustly Improve Multilingual Encoders. EMNLP 2020.</li>
	  <li>Rethinking attention with performers. [<a href="https://drive.google.com/file/d/17iN_wceIKdaPpU1hG3UNDjtkYj7lwE8x/view?usp=sharing" target="#">slides</a>]</li>
	  </ul>
	  </td>
  </tr>

  <tr>
	<td>28 October</td>
	  <td>Hossein Mohebbi, Mohammad Ali Modaresi</td>
	  <td>
	  <ul>
	  <li>A Tale of a Probe and a Parser. ACL 2020.</li>
 	  <li>What Do Position Embeddings Learn? An Empirical Study of Pre-Trained Language Model Positional Encoding. EMNLP 2020.</li>
	  </ul>
	  </td>
  </tr>

  <tr>
	<td>21 October</td>
	  <td>Sara Rajaee, Samin Fatehi, Mahsa Razavi</td>
	  <td>
	  <ul>
	  <li>BERT-EMD: Many-to-Many Layer Mapping for BERT Compression with Earth Mover's Distance. EMNLP 2020. [<a href="https://drive.google.com/file/d/1JUcs9BVayf9uTEIId-2yelL0sYqRRAx2/view?usp=sharing" target="#">slides</a>]</li>
 	  <li>Null It Out: Guarding Protected Attributes by Iterative Nullspace Projection. ACL 2020.</li>
	  <li>Multi-Turn Response Selection for Chatbots with Deep Attention Matching Network. ACL 2018.</li>
	  </ul>
	  </td>
  </tr>

  <tr>
	<td>14 October</td>
	  <td>Sara Rajaee, Maryam Sadat Hashemi</td>
	  <td>
	  <ul>
	  <li>Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them. NAACL 2019. [<a href="https://drive.google.com/file/d/17e_MlncTiuZrDFg8WA1SzwEwdO9sEh1B/view?usp=sharing" target="#">slides</a>]</li>
 	  <li>Cross-Modality Relevance for Reasoning on Language and Vision. ACL 2020. [<a href="https://drive.google.com/file/d/1tzWB07Oiyl7dZReSx6_Kgg1jBbNpDkia/view?usp=sharing" target="#">slides</a>]</li>
	  </ul>
	  </td>
  </tr>

  <tr>
	<td>7 October</td>
	  <td>Houman Mehrafarin, Kiamehr Rezaee, Mohsen Tabasi</td>
	  <td>
	  <ul>
	  <li>What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models. TACL 2020. [<a href="https://drive.google.com/file/d/1xeDb9Fu3tD1WaR95t1LC-UMiu4coazcx/view?usp=sharing" target="#">slides</a>]</li>
 	  <li>Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference. </li>
          <li>Breaking Through the 80% Glass Ceiling: Raising the State of the Art in Word Sense Disambiguation by Incorporating Knowledge Graph Information. ACL 2020.</li>
	  </ul>
	  </td>
  </tr>

  <tr>
	<td>30 September</td>
	  <td>Amin Pourdabiri, Maryam Sadat Hashemi, Samin Fatehi</td>
	  <td>
	  <ul>
	  <li>Big Bird: Transformers for Longer Sequences. [<a href="https://drive.google.com/file/d/1Ggvxa3TRkYenDXJJt7xeYDd1VmgOSE0G/view?usp=sharing" target="#">slides</a>]</li>
 	  <li>Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks. ECCV 2020. [<a href="https://drive.google.com/file/d/1voqJSWtLFzwtsyMiV0dbwY5QVdCsWcj5/view?usp=sharing" target="#">slides</a>]</li>
	  <li>Attention over Parameters for Dialogue Systems. NeurIPS 2020 ConvAI workshop. [<a href="https://drive.google.com/file/d/1QbdUsAOV7RyM6sur4gMIKIa-93xJ2c0e/view?usp=sharing" target="#">slides</a>]</li>
	  </ul>
	  </td>
  </tr>
   
  <tr>
	<td>23 September</td>
	  <td>Ali Modaresi and Hossein Mohebbi</td>
	  <td>
	  <ul>
	  <li>Quantifying Attention Flow in Transformers. ACL 2020. [<a href="https://drive.google.com/file/d/1la5ONPD2Ueg8Hx72OdvkWA5QYQ-9OokM/view?usp=sharing" target="#">slides</a>]</li>
 	  <li>DeFormer: Decomposing Pre-trained Transformers for Faster Question Answering. ACL 2020. [<a href="https://drive.google.com/file/d/1Q_1xdbiRXDylmNAZCkBKxZs99SJ3D4Rc/view?usp=sharing" target="#">slides</a>]</li>
	  </ul>
	  </td>

  </tr>

</tbody>
</table>

<h2> Summer 2020 </h2>

<table class="tg">
<thead>
  <tr>
    <th width="10%">Date</th>
    <th width="20%">Presenters</th>
    <th width="70%">Topic / Paper</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td>4 July</td>
    <td>Houman Mehrafarin, Hossein Mohebbi, Amin Pourdabiri, Ali Modaresi</td>
    <td>
     <ul>
        <li>Perturbed Masking: Parameter-free Probing for Analyzing and Interpreting BERT</li>
        <li>BERT Loses Patience: Fast and Robust Inference with Early Exit. [<a href="https://drive.google.com/file/d/1DKKOqFpJfJd8ontlwq3oYHqkLbkFB2Fp/view?usp=sharing" target="#">slides</a>]</li>
        <li>Accelerating Natural Language Understanding in Task-Oriented Dialog</li>
        <li>Fine-tune BERT with Sparse Self-Attention Mechanism. [<a href="https://drive.google.com/file/d/16M5_q7cX5p_1pvqteWYUBDb5v2P3KdbF/view?usp=sharing" target="#">slides</a>]</li>
     </ul>
     </td>
  </tr>
  <tr>
    <td>11 July</td>
    <td>Houman Mehrafarin, Sara Rajaee</td>
   <td><ul><li>Linguistic Knowledge and Transferability of Contextual Representations. [<a href="https://drive.google.com/file/d/1srXeXF_3mKJb73XtS67TBQqxh1h3utXx/view?usp=sharing" target="#">slides</a>]</li>
    <li>Finding Universal Grammatical Relations in Multilingual BERT. [<a href="https://drive.google.com/file/d/1RxMu8lVRPxZIm7c0bZCYxEDf8KqCVPr5/view?usp=sharing" target="#">slides</a>] </li></ul></td>
  </tr>

   <tr>
    <td>15 July</td>
    <td>Mohsen Tabasy, Kiamehr Rezaee, Zahra Sayedi</td>
    <td><ul><li>How does BERTâ€™s attention change when you fine-tune? An analysis methodology and a case study in negation scope. [<a href="https://drive.google.com/file/d/1iVjamQ09LffYPcyu6Y9JwuLUUGf4FGFm/view?usp=sharing" target="#">slides</a>]</li>
        <li>Similarity of Neural Network Representations Revisited</li>
    <li>Consistent Dialogue Generation with Self-supervised Feature Learning</li></ul></td>
  </tr>
 
  <tr>
    <td>22 July</td>
    <td>Hossein Mohebbi, Amin Pourdabiri, Ali Modaresi</td>
   <td><ul><li>BERT-of-Theseus: Compressing BERT by Progressive Module Replacing. [<a href="https://drive.google.com/file/d/1pHjNqun2PoRhn0Ar84D_v62G3BBPY7zR/view?usp=sharing" target="#">slides</a>]</li>
        <li>Beyond Domain APIs: Task-oriented Conversational Modeling with Unstructured Knowledge Access</li>
        <li>FastBERT: a Self-distilling BERT with Adaptive Inference Time. [<a href="https://drive.google.com/file/d/1swoewrBClZywNwyEAOK3fUNE6t3WJ-9o/view?usp=sharing" target="#">slides</a>]</li></td>
  </tr>
  <tr>
   <td>29 July </td>
   <td>Sara Rajaee, Maryam Sadat Hashemi, Samin Fatehi</td>
   <td><ul><li>On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines. [<a href="https://drive.google.com/file/d/1Wr8RECTpn5iGHHQ9FrKNcxR6Kkdicuhs/view?usp=sharing" taret="#">slides</a>]</li>
	   <li>VisualBERT: A Simple and Performant Baseline for Vision and Language. [<a href="https://drive.google.com/file/d/1890RMx5gfc7LePE0rx46KYpxaHNmzYkv/view?usp=sharing" taret="#">slides</a>]</li>
	   <li>Parameter-Efficient Transfer Learning for NLP. [<a href="https://drive.google.com/file/d/17L9ar4q-hR-Tb6-by0vJ7b-iL4T9uFNN/view?usp=sharing" taret="#">slides</a>]</li></ul></td>
  </tr>
  <tr>
	<td>5 August</td>
	  <td>Mohsen Tabasy, Kiamehr Rezaee, Zahra Sayedi</td>
	  <td><ul>
		<li>Explaining Black Box Predictions and Unveiling Data Artifacts through Influence Functions. [<a href="https://drive.google.com/file/d/1v3aSnIPSQbirkUOhYhYO3lP8rfO9b55_/view?usp=sharing" target="#">slides</a>]</li>
		<li>Beyond Accuracy: Behavioral Testing of NLP Models with CheckList</li>
		<li>You Impress Me: Dialogue Generation via Mutual Persona Perception</li>
		  </ul></td>
  </tr>
  <tr>
	  <td>12 August</td>
	  <td>Houman Mehrafarin, Hossein Mohebbi, Mohammad Ali Modarresi</td>
	  <td>
	  <ul>
	  <li>Probing Linguistic Systematicity, ACL 2020. [<a href="https://drive.google.com/file/d/1vm2yFV61tjw0bi90oXS0nOMJvW2lv41X/view?usp=sharing" target="#">slides</a>]</li>
	  <li>ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators, ICLR 2020. [<a href="https://drive.google.com/file/d/1epybyJdJpNjsm1yzIaRHzhgV6Q6omffX/view?usp=sharing" target="#">slides</a>]</li>
	  <li>BART: Denoising Sequence-to-Sequence Pre-training for Natural
Language Generation, Translation, and Comprehension, ACL 2020. [<a href="https://drive.google.com/file/d/1YMjdzQwwPXVQPhqmynfaU2J0A8Uh1eJZ/view?usp=sharing" target="#">slides</a>]</li>
	</ul>
	  </td>
  </tr>
    <tr>
	<td>19 August</td>
	  <td>Sara Rajaee, Samin Fatehi, Mahsa Razavi</td>
	  <td><ul>
		<li>A Mixture of h-1 Heads is Better than h Heads, ACL 2020. [<a href="https://drive.google.com/file/d/1-0JD8-ozDScxz9Lq1GPHuGaSs4wo8jof/view?usp=sharing" target="#">slides</a>]</li>
		<li>Longformer: The Long-Document Transformer, 2020. [<a href="https://drive.google.com/file/d/1zoYEHU8Q3KeNKc7YNGRiVt9tgNi-V8rm/view?usp=sharing" target="#">slides</a>]</li>
		<li>Transferable Multi-Domain State Generator for Task-Oriented Dialogue Systems, ACL 2019.</li>
	</ul></td>
  </tr>
     <tr>
	<td>26 August</td>
	  <td>Mohsen Tabasy, Kiamehr Rezaee, Amin Pourdabiri</td>
	  <td>
	  <ul>
	  <li>The Sensitivity of Language Models and Humans to Winograd Schema Perturbations, ACL 2020. [<a href="https://drive.google.com/file/d/18xYH-Q25ijBvRDbWQWgjNftYNLbdldYV/view?usp=sharing" target="#">slides</a>]
	  [<a href="https://github.com/m-tabasy/nlp_notebooks/blob/master/Inspect_mlm.ipynb" target="#">notebook</a>]
	  </li>
 	  <li>Learning to Speak and Act in a Fantasy Text Adventure Game. EMNLP 2019. [<a href="https://drive.google.com/file/d/1jImPx7btT_Axkxc2Nwtl_Nozv-AlRz4q/view?usp=sharing" target="#">slides</a>]</li>
  	  <li>Emerging Cross-lingual Structure in Pretrained Language Models. ACL 2020.</li>
	  </ul>
	  </td>
  </tr>
  
  <tr>
	<td>2 September</td>
	  <td>Amin Pourdabiri, Maryam Sadat Hashemi, Samin Fatehi</td>
	  <td>
	  <ul>
	  <li>Deploying Lifelong Open-Domain Dialogue Learning. [<a href="https://drive.google.com/file/d/13Uh_-VUfULMbw5AqZGgs6gJ3lGBkzMYh/view?usp=sharing" target="#">slides</a>]</li>
 	  <li>Reformer: The Efficient Transformer, ICLR 2020. [<a href="https://drive.google.com/file/d/1VItxdJzGghR5F3ihUEwvY2_MNpB1JoFg/view?usp=sharing" target="#">slides</a>]</li>
  	  <li>Revealing the Dark Secrets of BERT, EMNLP 2019. [<a href="https://drive.google.com/file/d/1jzfaRDIx0188O_a-he0AEJY-FU34JGw5/view?usp=sharing" target="#">slides</a>]</li>
	  </ul>
	  </td>
  </tr> 

  <tr>
	<td>9 September</td>
	  <td>Houman Mehrafarin, Kiamehr Rezaee</td>
	  <td>
	  <ul>
	  <li>Do Neural Language Models Show Prefrences for Syntactic Formalisms? ACL 2020. [<a href="https://drive.google.com/file/d/1y60DgzLxAUEsDtLLLDiamnMPrrxZmp1Y/view?usp=sharing" target="#">slides</a>]</li>
 	  <li>Multilingual Alignment of Contextual Word Representations. ICLR 2020.</li>
	  </ul>
	  </td>
  </tr>

  <tr>
	<td>16 September</td>
	  <td>Mohsen Tabasy, Maryam Sadat Hashemi, Sara Rajaee</td>
	  <td>
	  <ul>
	  <li>Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data. ACL 2020. [<a href="https://drive.google.com/file/d/1NZ31ldxOJNLoTS9fKMV2ENRNmcj79v59/view?usp=sharing" target="#">slides</a>]</li>
 	  <li>MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices. ACL 2020. [<a href="https://drive.google.com/file/d/1Hk74grw-XwFIoXl6KIRxrWYR7kzH2V30/view?usp=sharing" target="#">slides</a>]</li>
 	  <li>Knowledge Enhanced Contextual Word Representations. EMNLP 2019. [<a href="https://drive.google.com/file/d/1gJFaLAmBzxv8hCP2_XkgNoN8HK9OyrxJ/view?usp=sharing" target="#">slides</a>]</li>
	  </ul>
	  </td>
  </tr>
    
</tbody>
</table>

<br>

<h1>Series 1</h1>
<h2> Spring 2020 </h2>

<table class="tg">
<thead>
  <tr>
    <th width="10%">Date</th>
    <th width="20%">Presenters</th>
    <th width="70%">Topic / Paper</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td>20 April</td>
    <td>Mohsen Tabasy</td>
    <td>
    <ul><li>BERT Rediscovers the Classical NLP Pipeline. [<a href="https://drive.google.com/file/d/12TZyuArjU5EaCPshbOBH9RvdY0LT67G5/view?usp=sharing" taret="#">slides</a>]</li>
    <li>Universal Adversarial Triggers for Attacking and Analyzing NLP. [<a href="https://drive.google.com/file/d/12TZyuArjU5EaCPshbOBH9RvdY0LT67G5/view?usp=sharing" taret="#">slides</a>]</li>
    </ul></td>
  </tr>
  <tr>
    <td>27 April</td>
    <td>Hossein Mohebbi</td>
    <td>
    <ul><li>
    oLMpics -- On what Language Model Pre-training Captures
    </li>
    <li>Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping. [<a href="https://drive.google.com/file/d/10h6tOj-ceQfUNwcC4KynFllvDSRj-IWq/view?usp=sharing" target="#">slides</a>]</li></ul></td>
  </tr>
  <tr>
    <td>4 May</td>
    <td>Houman Mehrafarin</td>
    <td><ul><li>
    What Does BERT Look At? An Analysis of BERT's Attention. [<a href="https://drive.google.com/file/d/1ikvjY38DuyTM8uqKYZ0_Dw8nB-DCZXiy/view?usp=sharing" target="#">slides</a>]</li></ul></td>
  </tr>
  <tr>
    <td>11 May</td>
    <td>Amin Pourdabiri</td>
    <td><ul><li>Spying on your neighbors: Fine-grained probing of contextual embeddings for information about surrounding words</li></ul></td>
  </tr>
  <tr>
    <td>18 May</td>
    <td>Kiamehr Rezaee</td>
    <td><ul><li>How Multilingual is Multilingual BERT?</li></ul></td>
  </tr>
  <tr>
    <td>25 May</td>
    <td>-</td>
    <td>-</td>
  </tr>
  <tr>
    <td>1 June</td>
    <td>Ali Modaresi</td>
    <td>
    <ul>
    <li>BERT-based Lexical Substitution. [<a href="https://drive.google.com/file/d/11ztPI63a6VifHi_zhtNilYsb3ZoV75h8/view?usp=sharing" target="#">slides</a>]</li>
    <li>Contextual Embeddings: When Are They Worth It? [<a href="https://drive.google.com/file/d/1ww2ulinYZ4QlSCpiR87uzO_kweYaxfOH/view?usp=sharing" target="#">slides</a>]</li>
    </ul>
    </td>
  </tr>
</tbody>
</table>


