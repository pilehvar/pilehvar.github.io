<link href="https://fonts.googleapis.com/css2?family=Roboto&display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=Open+Sans:wght@300&family=Roboto&display=swap" rel="stylesheet">
<style type="text/css">
 .tg  {border-collapse:collapse;border-spacing:0;font-family:'Open Sans', sans-serif;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:'Open Sans',sans-serif;font-size:14px;
  overflow:hidden;padding:10px 15px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:'Open Sans',sans-serif;font-size:14px;
  font-weight:bold;overflow:hidden;padding:10px 15px;word-break:normal;}
</style>


<h1>Series 2</h1>
<h2> Summer 2020 </h2>


<h1>Series 1</h1>
<h2> Spring 2020 </h2>

<table class="tg">
<thead>
  <tr>
    <th>Date</th>
    <th>Moderator</th>
    <th>Topic / Paper</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td>20 April</td>
    <td>Mohsen Tabasy</td>
    <td><span style="font-weight:normal">BERT Rediscovers the Classical NLP Pipeline</span><br><span style="font-weight:400;font-style:normal">Universal Adversarial Triggers for Attacking and Analyzing NLP</span></td>
  </tr>
  <tr>
    <td>27 April</td>
    <td>Hossein Mohebbi</td>
    <td>oLMpics -- On what Language Model Pre-training Captures<br>Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping</td>
  </tr>
  <tr>
    <td>4 May</td>
    <td>Houman Mehrafarin</td>
    <td><span style="font-weight:400;font-style:normal">What Does BERT Look At? An Analysis of BERT's Attention</span></td>
  </tr>
  <tr>
    <td>11 May</td>
    <td>Amin Pourdabiri</td>
    <td><span style="font-weight:400;font-style:normal">Spying on your neighbors: Fine-grained probing of contextual embeddings for information about surrounding words</span></td>
  </tr>
  <tr>
    <td>18 May</td>
    <td>Kiamehr Rezaee</td>
    <td><span style="font-weight:400;font-style:normal">How Multilingual is Multilingual BERT?</span></td>
  </tr>
  <tr>
    <td>25 May</td>
    <td>-</td>
    <td>-</td>
  </tr>
  <tr>
    <td>1 June</td>
    <td>Ali Modaresi</td>
    <td>BERT-based Lexical Substitution<br>Contextual Embeddings: When Are They Worth It?</td>
  </tr>
</tbody>
</table>


