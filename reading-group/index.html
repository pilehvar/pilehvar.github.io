<link href="https://fonts.googleapis.com/css2?family=Roboto&display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=Open+Sans:wght@300&family=Roboto&display=swap" rel="stylesheet">
<style type="text/css">
 .tg  {border-collapse:collapse;border-spacing:0;font-family:'Open Sans', sans-serif;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:'Open Sans',sans-serif;font-size:14px;
  overflow:hidden;padding:10px 15px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:'Open Sans',sans-serif;font-size:14px;
  font-weight:bold;overflow:hidden;padding:10px 15px;word-break:normal;}
</style>

<h1>NLP Reading Group</h1>
<h2>Iran University of Science and Technology</h2>
<h3>Moderator: Mohammad Taher Pilehvar</h3>
<h3>Wednesdays, 11am-12pm (online on Teams)</h3>
<h4><i>Contact the moderator if you are interested in joining the group.</i></h4>

<hr>

<h1>Series 2</h1>
<h2> Summer 2020 </h2>

<table class="tg">
<thead>
  <tr>
    <th>Date</th>
    <th>Presenters</th>
    <th>Topic / Paper</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td>4 July</td>
    <td>Houman Mehrafarin, Hossein Mohebbi, Amin Pourdabiri, Ali Modaresi</td>
    <td>
     <ul>
        <li>Perturbed Masking: Parameter-free Probing for Analyzing and Interpreting BERT</li>
        <li>BERT Loses Patience: Fast and Robust Inference with Early Exit</li>
        <li>Accelerating Natural Language Understanding in Task-Oriented Dialog</li>
        <li>Fine-tune BERT with Sparse Self-Attention Mechanism</li>
     </ul>
     </td>
  </tr>
  <tr>
    <td>11 July</td>
    <td>Houman Mehrafarin, Sara Rajaee</td>
   <td><ul><li>Linguistic Knowledge and Transferability of Contextual Representations</li>
    <li>Finding Universal Grammatical Relations in Multilingual BERT</li></ul></td>
  </tr>

   <tr>
    <td>15 July</td>
    <td>Mohsen Tabasy, Kiamehr Rezaee, Zahra Sayedi</td>
    <td><ul><li>How does BERTâ€™s attention change when you fine-tune? An analysis methodology and a case study in negation scope</li>
        <li>Similarity of Neural Network Representations Revisited</li>
    <li>Consistent Dialogue Generation with Self-supervised Feature Learning</li></ul></td>
  </tr>
 
  <tr>
    <td>22 July</td>
    <td>Hossein Mohebbi, Amin Pourdabiri, Ali Modaresi</td>
   <td><ul><li>BERT-of-Theseus: Compressing BERT by Progressive Module Replacing</li>
        <li>Beyond Domain APIs: Task-oriented Conversational Modeling with Unstructured Knowledge Access</li>
        <li>FastBERT: a Self-distilling BERT with Adaptive Inference Time</li></td>
  </tr>
  <tr>
   <td>29 July </td>
   <td>TBD </td>
   <td>TBD </td>
  </tr>
</tbody>
</table>

<br>

<h1>Series 1</h1>
<h2> Spring 2020 </h2>

<table class="tg">
<thead>
  <tr>
    <th>Date</th>
    <th>Presenter</th>
    <th>Topic / Paper</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td>20 April</td>
    <td>Mohsen Tabasy</td>
    <td><span style="font-weight:normal">BERT Rediscovers the Classical NLP Pipeline</span><br><span style="font-weight:400;font-style:normal">Universal Adversarial Triggers for Attacking and Analyzing NLP</span></td>
  </tr>
  <tr>
    <td>27 April</td>
    <td>Hossein Mohebbi</td>
    <td>oLMpics -- On what Language Model Pre-training Captures<br>Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping</td>
  </tr>
  <tr>
    <td>4 May</td>
    <td>Houman Mehrafarin</td>
    <td><span style="font-weight:400;font-style:normal">What Does BERT Look At? An Analysis of BERT's Attention</span></td>
  </tr>
  <tr>
    <td>11 May</td>
    <td>Amin Pourdabiri</td>
    <td><span style="font-weight:400;font-style:normal">Spying on your neighbors: Fine-grained probing of contextual embeddings for information about surrounding words</span></td>
  </tr>
  <tr>
    <td>18 May</td>
    <td>Kiamehr Rezaee</td>
    <td><span style="font-weight:400;font-style:normal">How Multilingual is Multilingual BERT?</span></td>
  </tr>
  <tr>
    <td>25 May</td>
    <td>-</td>
    <td>-</td>
  </tr>
  <tr>
    <td>1 June</td>
    <td>Ali Modaresi</td>
    <td>BERT-based Lexical Substitution<br>Contextual Embeddings: When Are They Worth It?</td>
  </tr>
</tbody>
</table>


