<html>
<head>
  <title>NLP Reading Group :: IUST :: Mohammad Taher Pilehvar</title>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-125957901-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-125957901-1');
</script>


<link href="https://fonts.googleapis.com/css2?family=Roboto&display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=Open+Sans:wght@300&family=Roboto&display=swap" rel="stylesheet">
<style type="text/css">
 .tg  {border-collapse:collapse;border-spacing:0;font-family:'Open Sans', sans-serif;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:'Open Sans',sans-serif;font-size:14px;
  overflow:hidden;padding:10px 15px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:'Open Sans',sans-serif;font-size:14px;
  font-weight:bold;overflow:hidden;padding:10px 15px;word-break:normal;}
</style>
</head>

<body style="margin:20;padding:20">

<h1>NLP Reading Group</h1>
<h2>Iran University of Science and Technology</h2>
<h3>Moderator: Mohammad Taher Pilehvar</h3>
<h3>Wednesdays, 11am-12pm (online on Teams)</h3>
<h4><i>Contact the moderator if you are interested in joining the group.</i></h4>

<hr>

<h1>Series 2</h1>
<h2> Summer 2020 </h2>

<table class="tg">
<thead>
  <tr>
    <th>Date</th>
    <th>Presenters</th>
    <th>Topic / Paper</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td>4 July</td>
    <td>Houman Mehrafarin, Hossein Mohebbi, Amin Pourdabiri, Ali Modaresi</td>
    <td>
     <ul>
        <li>Perturbed Masking: Parameter-free Probing for Analyzing and Interpreting BERT</li>
        <li>BERT Loses Patience: Fast and Robust Inference with Early Exit. [<a href="https://drive.google.com/file/d/1DKKOqFpJfJd8ontlwq3oYHqkLbkFB2Fp/view?usp=sharing" target="#">slides</a>]</li>
        <li>Accelerating Natural Language Understanding in Task-Oriented Dialog</li>
        <li>Fine-tune BERT with Sparse Self-Attention Mechanism. [<a href="https://drive.google.com/file/d/16M5_q7cX5p_1pvqteWYUBDb5v2P3KdbF/view?usp=sharing" target="#">slides</a>]</li>
     </ul>
     </td>
  </tr>
  <tr>
    <td>11 July</td>
    <td>Houman Mehrafarin, Sara Rajaee</td>
   <td><ul><li>Linguistic Knowledge and Transferability of Contextual Representations. [<a href="https://drive.google.com/file/d/1srXeXF_3mKJb73XtS67TBQqxh1h3utXx/view?usp=sharing" target="#">slides</a>]</li>
    <li>Finding Universal Grammatical Relations in Multilingual BERT. [<a href="https://drive.google.com/file/d/1RxMu8lVRPxZIm7c0bZCYxEDf8KqCVPr5/view?usp=sharing" target="#">slides</a>] </li></ul></td>
  </tr>

   <tr>
    <td>15 July</td>
    <td>Mohsen Tabasy, Kiamehr Rezaee, Zahra Sayedi</td>
    <td><ul><li>How does BERTâ€™s attention change when you fine-tune? An analysis methodology and a case study in negation scope. [<a href="https://drive.google.com/file/d/1iVjamQ09LffYPcyu6Y9JwuLUUGf4FGFm/view?usp=sharing" target="#">slides</a>]</li>
        <li>Similarity of Neural Network Representations Revisited</li>
    <li>Consistent Dialogue Generation with Self-supervised Feature Learning</li></ul></td>
  </tr>
 
  <tr>
    <td>22 July</td>
    <td>Hossein Mohebbi, Amin Pourdabiri, Ali Modaresi</td>
   <td><ul><li>BERT-of-Theseus: Compressing BERT by Progressive Module Replacing. [<a href="https://drive.google.com/file/d/1pHjNqun2PoRhn0Ar84D_v62G3BBPY7zR/view?usp=sharing" target="#">slides</a>]</li>
        <li>Beyond Domain APIs: Task-oriented Conversational Modeling with Unstructured Knowledge Access</li>
        <li>FastBERT: a Self-distilling BERT with Adaptive Inference Time. [<a href="https://drive.google.com/file/d/1swoewrBClZywNwyEAOK3fUNE6t3WJ-9o/view?usp=sharing" target="#">slides</a>]</li></td>
  </tr>
  <tr>
   <td>29 July </td>
   <td>Sara Rajaee, Maryam Sadat Hashemi, Samin Fatehi</td>
   <td><ul><li>On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines. [<a href="https://drive.google.com/file/d/1Wr8RECTpn5iGHHQ9FrKNcxR6Kkdicuhs/view?usp=sharing" taret="#">slides</a>]</li>
	   <li>VisualBERT: A Simple and Performant Baseline for Vision and Language. [<a href="https://drive.google.com/file/d/1890RMx5gfc7LePE0rx46KYpxaHNmzYkv/view?usp=sharing" taret="#">slides</a>]</li>
	   <li>Parameter-Efficient Transfer Learning for NLP. [<a href="https://drive.google.com/file/d/17L9ar4q-hR-Tb6-by0vJ7b-iL4T9uFNN/view?usp=sharing" taret="#">slides</a>]</li></ul></td>
  </tr>
  <tr>
	<td>5 August</td>
	  <td>Mohsen Tabasy, Kiamehr Rezaee, Zahra Sayedi</td>
	  <td><ul>
		<li>Explaining Black Box Predictions and Unveiling Data Artifacts through Influence Functions. [<a href="https://drive.google.com/file/d/1v3aSnIPSQbirkUOhYhYO3lP8rfO9b55_/view?usp=sharing" target="#">slides</a>]</li>
		<li>Beyond Accuracy: Behavioral Testing of NLP Models with CheckList</li>
		<li>You Impress Me: Dialogue Generation via Mutual Persona Perception</li>
		  </ul></td>
  </tr>
  <tr>
	  <td>12 August</td>
	  <td>Houman Mehrafarin, Hossein Mohebbi, Mohammad Ali Modarresi</td>
	  <td>
	  <ul>
	  <li>Probing Linguistic Systematicity, ACL 2020. [<a href="https://drive.google.com/file/d/1vm2yFV61tjw0bi90oXS0nOMJvW2lv41X/view?usp=sharing" target="#">slides</a>]</li>
	  <li>ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators, ICLR 2020. [<a href="https://drive.google.com/file/d/1epybyJdJpNjsm1yzIaRHzhgV6Q6omffX/view?usp=sharing" target="#">slides</a>]</li>
	  <li>BART: Denoising Sequence-to-Sequence Pre-training for Natural
Language Generation, Translation, and Comprehension, ACL 2020. [<a href="https://drive.google.com/file/d/1YMjdzQwwPXVQPhqmynfaU2J0A8Uh1eJZ/view?usp=sharing" target="#">slides</a>]</li>
	</ul>
	  </td>
  </tr>
    <tr>
	<td>19 August</td>
	  <td>Sara Rajaee, Samin Fatehi, Mahsa Razavi</td>
	  <td><ul>
		<li>A Mixture of h-1 Heads is Better than h Heads, ACL 2020.</li>
		<li>Longformer: The Long-Document Transformer, 2020. [<a href="https://drive.google.com/file/d/1zoYEHU8Q3KeNKc7YNGRiVt9tgNi-V8rm/view?usp=sharing" target="#">slides</a>]</li>
		<li>Transferable Multi-Domain State Generator for Task-Oriented Dialogue Systems, ACL 2019.</li>
	</ul></td>
  </tr>
     <tr>
	<td>26 August</td>
	  <td>Mohsen Tabasy, Kiamehr Rezaee, Amin Pourdabiri</td>
	  <td>
	  <ul>
	  <li>The Sensitivity of Language Models and Humans to Winograd Schema Perturbations, ACL 2020. [<a href="https://drive.google.com/file/d/18xYH-Q25ijBvRDbWQWgjNftYNLbdldYV/view?usp=sharing" target="#">slides</a>]
	  [<a href="https://github.com/m-tabasy/nlp_notebooks/blob/master/Inspect_mlm.ipynb" target="#">notebook</a>]
	  </li>
 	  <li>Learning to Speak and Act in a Fantasy Text Adventure Game. EMNLP 2019.</li>
  	  <li>Emerging Cross-lingual Structure in Pretrained Language Models. ACL 2020.</li>
	  </ul>
	  </td>
  </tr>
  
  <tr>
	<td>2 September</td>
	  <td>Amin Pourdabiri, Maryam Sadat Hashemi, Samin Fatehi</td>
	  <td>
	  <ul>
	  <li>Deploying Lifelong Open-Domain Dialogue Learning. </li>
 	  <li>Reformer: The Efficient Transformer, ICLR 2020. [<a href="https://drive.google.com/file/d/1VItxdJzGghR5F3ihUEwvY2_MNpB1JoFg/view?usp=sharing" target="#">slides</a>]</li>
  	  <li>Revealing the Dark Secrets of BERT, EMNLP 2019. [<a href="https://drive.google.com/file/d/1jzfaRDIx0188O_a-he0AEJY-FU34JGw5/view?usp=sharing" target="#">slides</a>]</li>
	  </ul>
	  </td>
  </tr> 

  <tr>
	<td>9 September</td>
	  <td>Houman Mehrafarin, Kiamehr Rezaee</td>
	  <td>
	  <ul>
	  <li>Do Neural Language Models Show Prefrences for Syntactic Formalisms? ACL 2020. </li>
 	  <li>Multilingual Alignment of Contextual Word Representations. ICLR 2020.</li>
	  </ul>
	  </td>
  </tr>

  <tr>
	<td>16 September</td>
	  <td>Mohsen Tabasy, Maryam Sadat Hashemi, Sara Rajaee</td>
	  <td>
	  <ul>
	  <li>Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data. ACL 2020.</li>
 	  <li>MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices. ACL 2020.</li>
 	  <li>Knowledge Enhanced Contextual Word Representations. EMNLP 2019.</li>

	  </ul>
	  </td>
  </tr>
    
</tbody>
</table>

<br>

<h1>Series 1</h1>
<h2> Spring 2020 </h2>

<table class="tg">
<thead>
  <tr>
    <th>Date</th>
    <th>Presenter</th>
    <th>Topic / Paper</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td>20 April</td>
    <td>Mohsen Tabasy</td>
    <td>
    <ul><li>BERT Rediscovers the Classical NLP Pipeline. [<a href="https://drive.google.com/file/d/12TZyuArjU5EaCPshbOBH9RvdY0LT67G5/view?usp=sharing" taret="#">slides</a>]</li>
    <li>Universal Adversarial Triggers for Attacking and Analyzing NLP. [<a href="https://drive.google.com/file/d/12TZyuArjU5EaCPshbOBH9RvdY0LT67G5/view?usp=sharing" taret="#">slides</a>]</li>
    </ul></td>
  </tr>
  <tr>
    <td>27 April</td>
    <td>Hossein Mohebbi</td>
    <td>
    <ul><li>
    oLMpics -- On what Language Model Pre-training Captures
    </li>
    <li>Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping. [<a href="https://drive.google.com/file/d/10h6tOj-ceQfUNwcC4KynFllvDSRj-IWq/view?usp=sharing" target="#">slides</a>]</li></ul></td>
  </tr>
  <tr>
    <td>4 May</td>
    <td>Houman Mehrafarin</td>
    <td><ul><li>
    What Does BERT Look At? An Analysis of BERT's Attention. [<a href="https://drive.google.com/file/d/1ikvjY38DuyTM8uqKYZ0_Dw8nB-DCZXiy/view?usp=sharing" target="#">slides</a>]</li></ul></td>
  </tr>
  <tr>
    <td>11 May</td>
    <td>Amin Pourdabiri</td>
    <td><ul><li>Spying on your neighbors: Fine-grained probing of contextual embeddings for information about surrounding words</li></ul></td>
  </tr>
  <tr>
    <td>18 May</td>
    <td>Kiamehr Rezaee</td>
    <td><ul><li>How Multilingual is Multilingual BERT?</li></ul></td>
  </tr>
  <tr>
    <td>25 May</td>
    <td>-</td>
    <td>-</td>
  </tr>
  <tr>
    <td>1 June</td>
    <td>Ali Modaresi</td>
    <td>
    <ul>
    <li>BERT-based Lexical Substitution. [<a href="https://drive.google.com/file/d/11ztPI63a6VifHi_zhtNilYsb3ZoV75h8/view?usp=sharing" target="#">slides</a>]</li>
    <li>Contextual Embeddings: When Are They Worth It? [<a href="https://drive.google.com/file/d/1ww2ulinYZ4QlSCpiR87uzO_kweYaxfOH/view?usp=sharing" target="#">slides</a>]</li>
    </ul>
    </td>
  </tr>
</tbody>
</table>


